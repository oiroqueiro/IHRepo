{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using state Galicia server backend.\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "\n",
    "#Connect to mysql\n",
    "import mysql.connector\n",
    "\n",
    "#Operating system\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "#import subprocess\n",
    "from datetime import timedelta\n",
    "\n",
    "#Transcription & subtitles\n",
    "import whisper\n",
    "import stable_whisper\n",
    "\n",
    "#Translation\n",
    "import translators as ts\n",
    "import translators.server as tss\n",
    "import textwrap\n",
    "\n",
    "#Pandas\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_ddbb():\n",
    "    \"\"\"Function to create the conection to the data base\n",
    "    \n",
    "    Keyword arguments:    \n",
    "    Return: connection objet, secrets object\n",
    "    \"\"\"\n",
    "       \n",
    "    secrets={}\n",
    "    #secrets_file = open('secrets.txt','r') #Had errors using this within a virtual environment\n",
    "    secrets_file = os.fdopen(os.open('secrets.txt', os.O_RDONLY))\n",
    "    for line in secrets_file:\n",
    "        (key, val) = line.replace('\\n','').split(\"|\")\n",
    "        secrets[key] = val    \n",
    "\n",
    "    #Conection to mysql\n",
    "\n",
    "    conn = mysql.connector.connect(user=secrets['user'],\n",
    "                            password=secrets['pass'],\n",
    "                            host=secrets['server'])\n",
    "    \n",
    "    return conn, secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection, secrets = connection_ddbb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ddbb(_conn,_secrets):\n",
    "    \"\"\"Function to create the data base\n",
    "    \n",
    "    Keyword arguments:  \n",
    "    _conn: connection object \n",
    "    _secrets: secrets object \n",
    "    Return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    #connection_ddbb\n",
    "    \n",
    "    #Creating schema in mysql\n",
    "\n",
    "    if _conn.is_connected():\n",
    "        cursor = _conn.cursor()\n",
    "\n",
    "        print('Connection open')        \n",
    "        \n",
    "        print('Creating database if necessary...')\n",
    "        \n",
    "        query = ('CREATE DATABASE IF NOT EXISTS ironrep')\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        _conn.commit()\n",
    "        \n",
    "        query = ('USE ironrep')\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        _conn.commit()\n",
    "        \n",
    "        print('Database created if necessary...')\n",
    "\n",
    "        print('Creating tables if necessary...')\n",
    "\n",
    "        #Configuration        \n",
    "        \n",
    "        query = (\"\"\"CREATE TABLE IF NOT EXISTS ironrep.configuration (  \n",
    "                        id enum('1') PRIMARY KEY NOT NULL,\n",
    "                        temp_directory  nvarchar(250),\n",
    "                        video_player nvarchar(250),\n",
    "                        languages_subtitles nvarchar(250) COMMENT 'List of langages codes separated by commas')\"\"\")\n",
    "\n",
    "        cursor.execute(query)    \n",
    "        _conn.commit()\n",
    "\n",
    "        #Default values\n",
    "        query = (\"\"\"REPLACE INTO ironrep.configuration (temp_directory,video_player,languages_subtitles)\n",
    "                        VALUES (%s,%s,%s)\"\"\")\n",
    "        val = (str(_secrets['temp_dir']),str(_secrets['video_play']),str(_secrets['lang_subt']))\n",
    "        \"\"\"val = (str('/home/roque/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/data/'),\n",
    "                str(\"vlc '{videoparam}' --sub-file '{subtitleparam}' --no-sub-autodetect-file --start-time '{positionparam}'\"),\n",
    "                str('es,pt,it,zh,de,hi'))\"\"\"\n",
    "        cursor.execute(query,val)\n",
    "        _conn.commit()\n",
    "\n",
    "        #Videos\n",
    "\n",
    "        query = (\"\"\"CREATE TABLE IF NOT EXISTS ironrep.videos (\n",
    "                        id INT PRIMARY KEY NOT NULL AUTO_INCREMENT,\n",
    "                        video nvarchar(250),\n",
    "                        video_path nvarchar(250)\n",
    "                    )\"\"\"\n",
    "        )\n",
    "\n",
    "        cursor.execute(query)    \n",
    "        _conn.commit()\n",
    "\n",
    "        #Transcriptions\n",
    "\n",
    "        query = (\"\"\"CREATE TABLE IF NOT EXISTS ironrep.transcriptions (\n",
    "                        id INT PRIMARY KEY NOT NULL AUTO_INCREMENT,\n",
    "                        videoid INT NOT NULL,\n",
    "                        languageid nvarchar(5),\n",
    "                        transcription mediumtext\n",
    "                    )\"\"\"\n",
    "        )\n",
    "\n",
    "        cursor.execute(query)    \n",
    "        _conn.commit()\n",
    "\n",
    "        #Summaries\n",
    "\n",
    "        query = (\"\"\"CREATE TABLE IF NOT EXISTS ironrep.summaries (\n",
    "                        videoid INT NOT NULL,\n",
    "                        languageid nvarchar(5),\n",
    "                        summary nvarchar(260)                   \n",
    "                    )\"\"\"\n",
    "        )\n",
    "\n",
    "        cursor.execute(query)    \n",
    "        _conn.commit()\n",
    "\n",
    "        #Subtitles\n",
    "\n",
    "        query = (\"\"\"CREATE TABLE IF NOT EXISTS ironrep.subtitles (\n",
    "                        id INT PRIMARY KEY NOT NULL AUTO_INCREMENT,\n",
    "                        videoid INT NOT NULL,\n",
    "                        languageid nvarchar(5),\n",
    "                        subtitles mediumtext                   \n",
    "                    )\"\"\"\n",
    "        )\n",
    "\n",
    "        cursor.execute(query)    \n",
    "        _conn.commit()\n",
    "\n",
    "        #Keywords\n",
    "\n",
    "        query = (\"\"\"CREATE TABLE IF NOT EXISTS ironrep.keywords (\n",
    "                        id INT PRIMARY KEY NOT NULL AUTO_INCREMENT,\n",
    "                        videoid INT NOT NULL,\n",
    "                        languageid nvarchar(5),\n",
    "                        keywords nvarchar(250)                   \n",
    "                    )\"\"\"\n",
    "        )\n",
    "\n",
    "        cursor.execute(query)    \n",
    "        _conn.commit()\n",
    "\n",
    "        print('Tables created if necessary...')\n",
    "    else:\n",
    "        print('Error connecting')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection open\n",
      "Creating database if necessary...\n",
      "Database created if necessary...\n",
      "Creating tables if necessary...\n",
      "Tables created if necessary...\n"
     ]
    }
   ],
   "source": [
    "create_ddbb(connection, secrets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_sql(_conn, _table, _videoid, _langid, _text):\n",
    "    \"\"\"Function to insert data in the database\n",
    "    \n",
    "    Keyword arguments:\n",
    "    _table: the object where insert ['video','transcription','subtitle','summary','keywords']\n",
    "    _videoid: the id of the video\n",
    "    _langid: the id of the language\n",
    "    _text: value to insert\n",
    "    Return: cursor\n",
    "    \"\"\"\n",
    "    \n",
    "    if _conn.is_connected():\n",
    "        cursor = _conn.cursor()\n",
    "        query = ''\n",
    "        val = []\n",
    "\n",
    "        if (_table == 'video'):                 \n",
    "            vid_path = os.path.split(os.path.abspath(Path(_text)))\n",
    "\n",
    "            query = \"\"\"INSERT INTO ironrep.videos(video,video_path)\n",
    "                        VALUES (%s,%s)\"\"\"\n",
    "            val = [vid_path[1].split('.')[0],_text]\n",
    "        elif (_table == 'transcription'):\n",
    "            query = \"\"\"INSERT INTO ironrep.transcriptions(videoid,languageid,transcription)\n",
    "                        VALUES (%s,%s,%s)\"\"\"\n",
    "            val = [int(_videoid),_langid,_text]\n",
    "        elif (_table == 'subtitle'):\n",
    "            query = \"\"\"INSERT INTO ironrep.subtitles(videoid,languageid,subtitles)\n",
    "                        VALUES (%s,%s,%s)\"\"\"\n",
    "            val = [int(_videoid),_langid,_text]\n",
    "        elif (_table == 'summary'):\n",
    "            query = \"\"\"INSERT INTO ironrep.summary(videoid,languageid,summary)\n",
    "                        VALUES (%s,%s,%s)\"\"\"\n",
    "            val = [int(_videoid),_langid,_text]\n",
    "        elif (_table == 'keywords'):\n",
    "            query = \"\"\"INSERT INTO ironrep.keywords(videoid,languageid,keywords)\n",
    "                        VALUES (%s,%s,%s)\"\"\"\n",
    "            val = [int(_videoid),_langid,_text]\n",
    "        else:\n",
    "            return 'none'\n",
    "\n",
    "        if (query != ''):      \n",
    "            cursor.execute(query,val)\n",
    "            _conn.commit()  \n",
    "            return cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(_conn, filepath):\n",
    "    \"\"\"\n",
    "    With this function we can transcribe all the texts from a video/audio and also the subtitles\n",
    "    \n",
    "    Keyword arguments:\n",
    "    argument -- filepath:the file to transcribe\n",
    "    Return: No return (insert in mysql and create 2 text files -temporary-)\n",
    "    \"\"\"\n",
    "    \n",
    "    video = os.path.split(os.path.abspath(Path(filepath)))\n",
    "    name = video[1].split(sep='.')\n",
    "\n",
    "    result_sql = insert_data_sql(_conn,'video', '', '', filepath)\n",
    "    videoid = result_sql.lastrowid\n",
    "    \n",
    "    # speech transcription\n",
    "    \n",
    "    model = whisper.load_model(\"base.en\",device='cpu')\n",
    "    #model = stable_whisper.load_model('base')\n",
    "\n",
    "    result = model.transcribe(filepath)\n",
    "    \n",
    "    with open(Path(video[0]+\"/\"+name[0]+\"_transcription.txt\"), \"w+\") as f:\n",
    "        f.write(result[\"text\"])\n",
    "\n",
    "    result_sql = insert_data_sql(_conn,'transcription', videoid, 'en', result['text'])\n",
    "\n",
    "    # subtitles \n",
    "    stable_whisper.results_to_sentence_srt(result, video[0]+\"/\"+name[0]+\"_subtitles.srt\")\n",
    "\n",
    "    text_subtitles = open(video[0]+\"/\"+name[0]+\"_subtitles.srt\").read()\n",
    "\n",
    "    result_sql = insert_data_sql(_conn,'subtitle', videoid, 'en', text_subtitles)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_subtitles(_conn, _videoid):\n",
    "    if _conn.is_connected():\n",
    "\n",
    "        cursor_conf = connection.cursor(buffered=True)\n",
    "\n",
    "        query_conf = \"\"\"SELECT languages_subtitles, temp_directory \n",
    "                        FROM ironrep.configuration\n",
    "                        LIMIT 1;\"\"\"\n",
    "        cursor_conf.execute(query_conf)\n",
    "\n",
    "        conf_table = cursor_conf.fetchall()\n",
    "        conf_df = pd.DataFrame(conf_table)\n",
    "        conf_df.columns = [i[0] for i in cursor_conf.description]\n",
    "\n",
    "        cursor = connection.cursor(buffered=True)\n",
    "        query = \"\"\"SELECT videoid, languageid, subtitles\n",
    "                    FROM ironrep.subtitles\n",
    "                    WHERE videoid = %s and languageid = %s\"\"\"\n",
    "        val = [int(_videoid),str('en')]\n",
    "        cursor.execute(query,val)\n",
    "        \n",
    "        subt_table = cursor.fetchall()\n",
    "        subt_df = pd.DataFrame(subt_table)\n",
    "        subt_df.columns = [i[0] for i in cursor.description]\n",
    "        \n",
    "        translated = []\n",
    "        text_subtitles = ''\n",
    "\n",
    "        for lang in [language for languages in conf_df['languages_subtitles'].str.split(',') for language in languages]:\n",
    "            for sub in subt_table:\n",
    "                print(lang)\n",
    "                #for row in sub[2].split('\\n'):\n",
    "                #    translated.append(translate_from_en(row,lang))\n",
    "                    #translated.append(row)\n",
    "                \n",
    "            with open(Path(list(conf_df['temp_directory'])[0]+\"/\"+lang+\"_subtitle_tmp.srt\"), mode='wt', encoding='utf-8') as f:    \n",
    "                f.write('\\n'.join(translated))\n",
    "\n",
    "            text_subtitles = open(Path(list(conf_df['temp_directory'])[0]+\"/\"+lang+\"_subtitle_tmp.srt\")).read().replace(' -> ',' --> ').replace(': ',':')\n",
    "\n",
    "            result_sql = insert_data_sql(connection,'subtitle', _videoid, lang, text_subtitles)\n",
    "\n",
    "            os.remove(Path(list(conf_df['temp_directory'])[0]+\"/\"+lang+\"_subtitle_tmp.srt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "pt\n",
      "it\n",
      "zh\n",
      "de\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "translate_subtitles(connection, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_player(_conn,_videoid, _langid, _position = 0):\n",
    "    \"\"\"Function to launch the video with subtitles\n",
    "    \n",
    "    Keyword arguments:\n",
    "    _conn: connection object\n",
    "    _videoid: the id of the video\n",
    "    _langid: the id of the language to use for the subtitles\n",
    "    _position: time in seconds to start the video\n",
    "    Return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor = _conn.cursor() \n",
    "    query = \"\"\"SELECT video_player, temp_directory\n",
    "                        FROM ironrep.configuration \n",
    "                        LIMIT 1;\"\"\"\n",
    "\n",
    "    cursor.execute(query)\n",
    "    conf_table = cursor.fetchall()\n",
    "    conf_df = pd.DataFrame(conf_table)\n",
    "    conf_df.columns = [i[0] for i in cursor.description]\n",
    "\n",
    "    query = \"\"\"SELECT subtitles\n",
    "                    FROM ironrep.subtitles\n",
    "                WHERE videoid = %s\n",
    "                    AND languageid = %s\"\"\"\n",
    "    val = [int(_videoid), _langid]\n",
    "    cursor.execute(query, val)    \n",
    "    subt_table = cursor.fetchall()\n",
    "    if (len(subt_table)>0):\n",
    "        subt_df = pd.DataFrame(subt_table)\n",
    "        subt_df.columns = [i[0] for i in cursor.description]\n",
    "\n",
    "        with open(Path(list(conf_df['temp_directory'])[0]+\"/play_subtitle.srt\"), \"w+\") as f:\n",
    "                f.write(list(subt_df['subtitles'])[0])\n",
    "\n",
    "        query = \"\"\"SELECT video_path\n",
    "                    FROM ironrep.videos\n",
    "                    WHERE id = %s\"\"\"\n",
    "        val = [int(_videoid)]\n",
    "        cursor.execute(query, val)\n",
    "        video_table = cursor.fetchall()\n",
    "        video_df = pd.DataFrame(video_table)\n",
    "        video_df.columns = [i[0] for i in cursor.description]\n",
    "\n",
    "        os.system(list(conf_df['video_player'])[0].replace('{videoparam}',list(video_df['video_path'])[0]).replace('{subtitleparam}',list(conf_df['temp_directory'])[0]+\"/play_subtitle.srt\").replace('{positionparam}',str(_position)))\n",
    "    else:\n",
    "         print(f'No subtitles found in language {_langid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00005649fe77d550] main libvlc: Ejecutar vlc con la interfaz predeterminada. Use «cvlc» para usar vlc sin interfaz.\n",
      "[00007f4edc002bf0] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
      "libva info: VA-API version 1.16.0\n",
      "libva error: vaGetDriverNameByIndex() failed with unknown libva error, driver_name = (null)\n",
      "[00007f4edc002bf0] glconv_vaapi_x11 gl error: vaInitialize: unknown libva error\n",
      "libva info: VA-API version 1.16.0\n",
      "libva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so\n",
      "libva info: Found init function __vaDriverInit_1_14\n",
      "libva error: /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so init failed\n",
      "libva info: va_openDriver() returns 1\n",
      "libva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/i965_drv_video.so\n",
      "libva info: Found init function __vaDriverInit_1_10\n",
      "libva info: va_openDriver() returns 0\n",
      "[00007f4edc002bf0] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
      "Failed to open VDPAU backend libvdpau_nvidia.so: no se puede abrir el archivo del objeto compartido: No existe el archivo o el directorio\n",
      "[00007f4edc45d000] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
      "[00007f4e9255c0d0] main decoder error: Timestamp conversion failed (delay 1000000, buffering 100000, bound 9000000)\n",
      "[00007f4e9255c0d0] main decoder error: Could not convert timestamp 97906400238 for FFmpeg\n",
      "[00007f4e925b7e60] main decoder error: Timestamp conversion failed for 5560384001: no reference clock\n",
      "[00007f4e925b7e60] main decoder error: Could not convert timestamp 0 for faad\n",
      "[00007f4e9255c0d0] main decoder error: Timestamp conversion failed for 5560440001: no reference clock\n",
      "[00007f4e9255c0d0] main decoder error: Could not convert timestamp 0 for FFmpeg\n",
      "[00007f4e92b84060] main decoder error: Timestamp conversion failed for 5532600001: no reference clock\n",
      "[00007f4e92b84060] main decoder error: Could not convert timestamps 0, 0 for Subtitles\n",
      "[00007f4e9255c0d0] main decoder error: Timestamp conversion failed (delay 1000000, buffering 100000, bound 9000000)\n",
      "[00007f4e9255c0d0] main decoder error: Could not convert timestamp 97916932692 for FFmpeg\n",
      "[00007f4e9255c0d0] main decoder error: Timestamp conversion failed (delay 1000000, buffering 100000, bound 9000000)\n",
      "[00007f4e9255c0d0] main decoder error: Could not convert timestamp 97928276367 for FFmpeg\n",
      "[00007f4e9255c0d0] main decoder error: Timestamp conversion failed (delay 1000000, buffering 100000, bound 9000000)\n",
      "[00007f4e9255c0d0] main decoder error: Could not convert timestamp 97921067883 for FFmpeg\n",
      "[h264 @ 0x7f4e92ab5000] get_buffer() failed\n",
      "[h264 @ 0x7f4e92ab5000] thread_get_buffer() failed\n",
      "[h264 @ 0x7f4e92ab5000] decode_slice_header error\n",
      "[h264 @ 0x7f4e92ab5000] get_buffer() failed\n",
      "[h264 @ 0x7f4e92ab5000] thread_get_buffer() failed\n",
      "[h264 @ 0x7f4e92ab5000] decode_slice_header error\n",
      "[h264 @ 0x7f4e92ab5000] get_buffer() failed\n",
      "[h264 @ 0x7f4e92ab5000] thread_get_buffer() failed\n",
      "[h264 @ 0x7f4e92ab5000] decode_slice_header error\n",
      "[h264 @ 0x7f4e92ab5000] get_buffer() failed\n",
      "[h264 @ 0x7f4e92ab5000] thread_get_buffer() failed\n",
      "[h264 @ 0x7f4e92ab5000] decode_slice_header error\n",
      "[h264 @ 0x7f4e92ab5000] no frame!\n",
      "[00007f4e9255c0d0] main decoder error: Timestamp conversion failed (delay 1000000, buffering 100000, bound 9000000)\n",
      "[00007f4e9255c0d0] main decoder error: Could not convert timestamp 97928692156 for FFmpeg\n",
      "[00007f4e9255c0d0] main decoder error: Timestamp conversion failed (delay 1000000, buffering 100000, bound 9000000)\n",
      "[00007f4e9255c0d0] main decoder error: Could not convert timestamp 97953692491 for FFmpeg\n",
      "[00007f4e92b84060] main decoder error: Timestamp conversion failed for 7448600001: no reference clock\n",
      "[00007f4e92b84060] main decoder error: Could not convert timestamps 0, 0 for Subtitles\n",
      "[00007f4e9255c0d0] main decoder error: Timestamp conversion failed for 7453000001: no reference clock\n",
      "[00007f4e9255c0d0] main decoder error: Could not convert timestamp 0 for FFmpeg\n",
      "[00007f4e92b84060] main decoder error: Timestamp conversion failed for 7628600001: no reference clock\n",
      "[00007f4e92b84060] main decoder error: Could not convert timestamps 0, 0 for Subtitles\n",
      "[00007f4e9255c0d0] main decoder error: Timestamp conversion failed for 7632360001: no reference clock\n",
      "[00007f4e9255c0d0] main decoder error: Could not convert timestamp 0 for FFmpeg\n",
      "\u001b[31muint DBusMenuExporterDBus::GetLayout(int, int, const QStringList&, DBusMenuLayoutItem&)\u001b[0m: Condition failed: menu\n",
      "QObject::~QObject: Timers cannot be stopped from another thread\n"
     ]
    }
   ],
   "source": [
    "video_player(connection, 52, 'en', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/25. Week 9 - Day 1/recordings/virtual_environments_anaconda_tutorial on Vimeo.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/26. Week 9 - Day 2/recordings/GMT20221103-174403_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/27. Week 9 - Day 3/recordings/GMT20221105-095041_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/27. Week 9 - Day 3/recordings/GMT20221105-130243_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/28. Week 10 - Day 1/recordings/GMT20221108-174016_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/29. Week 10 - Day 2/recordings/GMT20221110-174206_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/30. Week 10 - Day 3/recordings/GMT20221112-095150_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/31. Week 11 - Day 1/recordings/GMT20221115-174256_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/32. Week 11 - Day 2/recordings/GMT20221117-174613_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/33. Week 11 - Day 3/recordings/GMT20221119-095525_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/33. Week 11 - Day 3/recordings/GMT20221119-130318_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/34. Week 12 - Day 1/recordings/GMT20221122-174530_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/35. Week 12 - Day 2/recordings/GMT20221124-174853_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/36. Week 12 - Day 3/recordings/GMT20221126-100149_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/39. Week 13 - Day 3/recordings/GMT20221203-100111_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/40. Week 14 - Day 1/recordings/GMT20221206-174541_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/41. Week 14 - Day 2/recordings/GMT20221208-174524_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/42. Week 14 - Day 3/recordings/GMT20220514-084037_Recording_2560x1440.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/43. Week 15 - Day 1/recordings/GMT20221213-173613_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/44. Week 15 - Day 2/recordings/GMT20221215-174748_Recording_1920x1080.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/45. Week 15 - Day 3/recordings/GMT20221217-100543_Recording_2560x1440.mp4')\n",
    "transcribe(connection,'/home/roque/01. IronHack/00. Data Analytics/01. Course/45. Week 15 - Day 3/recordings/GMT20221222-173401_Recording_1920x1120.mp4')\n",
    "\n",
    "translate_subtitles(connection, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "# TODO: Try with a larger text\n",
    "\n",
    "# python -m spacy download en_core_web_sm #eficency\n",
    "# python -m spacy download en_core_web_trf #accuracy\n",
    "\n",
    "import spacy\n",
    "import en_core_web_trf\n",
    "#import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy_transformers\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "def summarize(text, per):\n",
    "    #nlp = spacy.load('en_core_web_trf')\n",
    "    nlp = en_core_web_trf.load()\n",
    "    doc= nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens= [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "    select_length=int(len(sentence_tokens)*per)\n",
    "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "    final_summary=[word.text for word in summary]\n",
    "    summary=''.join(final_summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(result['text'], 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like one main one main one main suggestion',\n",
       " 'whole thing like drives like applies force',\n",
       " '30 grammets per hour speed limits',\n",
       " 'insanely strong tech tech right behind',\n",
       " 'pretty high cpc cost per click',\n",
       " 'new race fund group b',\n",
       " 'analysis like drives towards towards',\n",
       " 'passengers involved involving car accident obviously',\n",
       " 'strong hypothesis like super well laid',\n",
       " 'getting like disparate data together like',\n",
       " 'regular two bars one one next',\n",
       " 'pretty intense machine learning technique',\n",
       " 'specific works like vhvac shit',\n",
       " 'thousand dollar investment per trade',\n",
       " 'barcelona like 50 speed limit pro',\n",
       " 'available like entire home apartment',\n",
       " 'data set time seven parameters',\n",
       " 'create something called budget persona',\n",
       " '46 per 46 values',\n",
       " 'transcutaneous vagus nerve stimulation',\n",
       " 'organic email marketing also influencer',\n",
       " 'entire home apartment type cost',\n",
       " 'think maybe 10 shapes something like',\n",
       " 'normally find hypothesis test hypothesis find',\n",
       " 'great like storytelling storytelling journey',\n",
       " 'counting 1003 accidents totally',\n",
       " 'korean korean blue band',\n",
       " '50 kilometers per hours',\n",
       " '1600 euro per month',\n",
       " 'higher price average listing price',\n",
       " 'like everything like every technical point',\n",
       " 'year bnb pricing factor dashboard',\n",
       " 'many data came much data came',\n",
       " 'complex midbook camp project',\n",
       " 'hang group equals false',\n",
       " 'serial dot zero five',\n",
       " 'successful professional life people could choose',\n",
       " 'seven point 88 total dollars',\n",
       " 'actual weather condition condition basically',\n",
       " 'neighborhood different average listings',\n",
       " 'progress takes place outside',\n",
       " 'pricing listing price also higher',\n",
       " 'mini boot camp project',\n",
       " 'variables like caffeine alcohol',\n",
       " 'one conversion one one sell',\n",
       " 'someone lost someone wins',\n",
       " 'new york city manhattan brooklyn',\n",
       " 'mid boot camp project',\n",
       " 'like normalization normalization work',\n",
       " 'room time room type also',\n",
       " 'different average pricing listing',\n",
       " 'specific called data dredging',\n",
       " 'congratulations folks anyone else wants',\n",
       " 'think like geospatial maps',\n",
       " 'one super small curiosity',\n",
       " 'second largest cinema chain',\n",
       " '56 decisions per block',\n",
       " 'took several pretty interesting conclusions',\n",
       " 'use kyke square testing',\n",
       " 'chosen three neighborhoods group',\n",
       " '30s rather small areas',\n",
       " 'natural language processing going',\n",
       " 'day mainly recording mainly accident',\n",
       " 'little bit behind schedule',\n",
       " 'nothing telling us either',\n",
       " 'average price per 90',\n",
       " 'manhattan average listing price',\n",
       " 'great great great great topic',\n",
       " 'statistical analysis like set hypothesis',\n",
       " 'inadequate information operating accommodation',\n",
       " 'final error five features',\n",
       " 'things like clinical trials',\n",
       " 'like every dashboard allows',\n",
       " 'super blocks planning basically',\n",
       " 'collected 93 data records',\n",
       " 'even though correlating everything',\n",
       " 'sham condition per day',\n",
       " 'someone receives positive consequences',\n",
       " 'like complex like everything',\n",
       " 'little technical components driving',\n",
       " 'three articles per day',\n",
       " 'speed limits data set',\n",
       " 'use like base theorem',\n",
       " 'top three cinema chain',\n",
       " 'overall data per day',\n",
       " 'used past market data',\n",
       " 'original big data set',\n",
       " 'two take home messages',\n",
       " 'much every click cost',\n",
       " 'regular physical activities',\n",
       " 'like average item price',\n",
       " 'multi lines probably road',\n",
       " 'power bi like like',\n",
       " 'user spend less energy',\n",
       " 'best compliment right like',\n",
       " 'nice 10 minutes exactly',\n",
       " 'actually pretty reasonable dashboard',\n",
       " 'actual road condition obviously',\n",
       " 'strong point influencing factor',\n",
       " 'clearly like two weeks',\n",
       " 'like super high marks',\n",
       " 'email influencer cause',\n",
       " 'first mentioned reinforcement learning',\n",
       " 'like super important coming',\n",
       " 'one group age range',\n",
       " 'thousand dollar investment',\n",
       " 'room type rating accommodation',\n",
       " 'available data sheet',\n",
       " 'entire home apartment',\n",
       " 'basically giving electric shocks',\n",
       " 'looks like super amazing',\n",
       " 'little bit data cleaning',\n",
       " 'little bit bullet point',\n",
       " 'even though average salary',\n",
       " 'profit revenue ratio basically',\n",
       " 'average listing price',\n",
       " 'unknown underlying mechanisms',\n",
       " 'mathematical computational models',\n",
       " 'administering atom moxetine',\n",
       " 'super professional looking dashboard',\n",
       " 'single day without articles',\n",
       " 'least one one step',\n",
       " 'work family consideration followed',\n",
       " 'highest average listing',\n",
       " 'spent per day',\n",
       " 'medium average ticket',\n",
       " 'bnb data set',\n",
       " 'least one presentation like',\n",
       " 'work coming back home',\n",
       " 'feel also male actress',\n",
       " 'every single square',\n",
       " 'less people feel successful',\n",
       " 'general perception pricing depends',\n",
       " 'two stimulation methods',\n",
       " 'chi square test',\n",
       " 'multi linear modeling',\n",
       " 'also like super nice',\n",
       " 'total cylindrical capacity',\n",
       " 'movie industry really hard',\n",
       " 'slightly behind schedule',\n",
       " 'prior bike handles',\n",
       " 'confident conference interval',\n",
       " 'particular type also like',\n",
       " 'different languages translated',\n",
       " 'looking forward equipment classes',\n",
       " 'successful professionals would make',\n",
       " 'higher rewards actually lead',\n",
       " 'basically totally sue',\n",
       " 'positive news articles released',\n",
       " 'investment per release',\n",
       " 'mid term presentation',\n",
       " 'market operating schedule',\n",
       " 'organic organic sales',\n",
       " 'location rating accommodation details',\n",
       " 'small average tickets',\n",
       " 'finding bright spots',\n",
       " 'clear hypothesis driven analysis',\n",
       " 'speed limits planning',\n",
       " 'seems like less content',\n",
       " 'find hypothesis test hypothesis',\n",
       " 'considering good students related',\n",
       " 'yet sufficiently practical',\n",
       " 'nurse whatsoever running',\n",
       " 'love new york',\n",
       " 'nice folks anyone wants',\n",
       " 'handmade jewelry online',\n",
       " 'high average ticket',\n",
       " 'super nice topic also',\n",
       " 'main data set',\n",
       " 'per word used',\n",
       " 'square contingency test',\n",
       " 'like mild severe',\n",
       " 'like default feedbacks',\n",
       " 'driving installation like',\n",
       " 'big broad range',\n",
       " 'charts actually like showcase',\n",
       " 'cause every month',\n",
       " 'super intelligent person',\n",
       " 'obviously materially impactful',\n",
       " 'essential design principles',\n",
       " 'different neighborhood group',\n",
       " 'different neighborhood group',\n",
       " 'used something called tv',\n",
       " 'post pandemic difficulties maybe',\n",
       " 'achieved educational level',\n",
       " 'know weather conditions based',\n",
       " 'something html css',\n",
       " 'digital nomads correlator',\n",
       " 'many different ages',\n",
       " 'extremely simple mind would',\n",
       " 'anyone else wants',\n",
       " 'chinese language chinese',\n",
       " 'high expectations lead',\n",
       " 'main like differences',\n",
       " '65 dollars theoretically',\n",
       " 'two neighborhood group',\n",
       " 'human factor cause',\n",
       " 'two different ways',\n",
       " 'two different blocks',\n",
       " 'need good data quality',\n",
       " 'every blue things',\n",
       " 'like strong notes',\n",
       " 'clear v shape',\n",
       " 'average annual incomes',\n",
       " 'smaller data set',\n",
       " 'limited data set',\n",
       " 'data cleaning translation',\n",
       " 'bullet point restating',\n",
       " 'different station life',\n",
       " 'specific human cause',\n",
       " 'used also positive correlation',\n",
       " 'average school grade',\n",
       " 'average school grade',\n",
       " 'news articles influence amazon',\n",
       " 'really congratulations like even',\n",
       " 'q square test',\n",
       " 'price average price',\n",
       " 'test one sided',\n",
       " 'different engenders population',\n",
       " 'roughly 55 minutes',\n",
       " 'account learning noise',\n",
       " 'robust predictive model',\n",
       " 'introduce technical data',\n",
       " 'finished fundamental studies',\n",
       " 'favorite actual actress',\n",
       " 'digital nomads longer',\n",
       " 'different reinforcement learning',\n",
       " 'less correlation among',\n",
       " 'new road planning',\n",
       " 'new calculated value',\n",
       " 'robust data gathering',\n",
       " '30 years old',\n",
       " 'whatsapp group chat',\n",
       " 'tightly connected across',\n",
       " 'technically complicated context',\n",
       " '30 second intervention',\n",
       " 'back million effects',\n",
       " 'spend every month',\n",
       " 'spend every month',\n",
       " 'like technical solidity',\n",
       " 'biggest driven year',\n",
       " 'subsequent professional success',\n",
       " 'whole data set',\n",
       " 'two ab testing',\n",
       " 'little bit stronger',\n",
       " 'little bit shape',\n",
       " 'little bit dying',\n",
       " 'power bi basically',\n",
       " 'think people feel good',\n",
       " '24 years old',\n",
       " '24 years old',\n",
       " '24 years old',\n",
       " 'recollecting 30 days',\n",
       " 'pc ppt point',\n",
       " 'email marketing strategy',\n",
       " 'large data set',\n",
       " 'large data set',\n",
       " 'bankruptcy list august',\n",
       " 'like urban planning',\n",
       " 'actual stimulation way',\n",
       " 'one sufficient data',\n",
       " 'interesting insights regarding gender',\n",
       " 'super block street',\n",
       " 'every single part',\n",
       " 'founded fantastic right',\n",
       " 'force like 4',\n",
       " 'title increased quite',\n",
       " 'achieve desired goals',\n",
       " 'top 10 songs',\n",
       " 'html stuff like',\n",
       " 'manhattan neighborhood group',\n",
       " 'two slides ago',\n",
       " 'online business idea',\n",
       " 'business online idea',\n",
       " 'task condition partial',\n",
       " 'yielding promising results',\n",
       " 'least five articles',\n",
       " 'several potential analysis',\n",
       " 'like correlates everything',\n",
       " 'new updated analysis',\n",
       " '10 minutes right',\n",
       " 'video stream revenues',\n",
       " 'last 20 %.',\n",
       " 'cinematographic industry grow',\n",
       " 'average ticket means',\n",
       " 'new societal change',\n",
       " 'quite advanced stuff',\n",
       " 'revenue dropped almost',\n",
       " 'bit like scared',\n",
       " 'cross relevant data',\n",
       " 'two percent presentation',\n",
       " 'also somewhat impressive',\n",
       " 'like really starving',\n",
       " 'less 70 rows',\n",
       " 'accidents took place',\n",
       " 'accidents took place',\n",
       " 'accidents took place',\n",
       " 'accidents took place',\n",
       " 'like super cool',\n",
       " 'less salary valued',\n",
       " 'box office front',\n",
       " 'trading either stocks',\n",
       " 'like next steps',\n",
       " 'many different stuff',\n",
       " 'top five movies',\n",
       " 'see like presentation thing',\n",
       " 'almost 10 years',\n",
       " 'pretty pretty solid',\n",
       " 'would good feet',\n",
       " 'hypothetical investment case',\n",
       " 'nice folks excellent presentation',\n",
       " 'granular details analysis',\n",
       " 'like alicia ed',\n",
       " 'html form like',\n",
       " 'getting like severely',\n",
       " 'also strong factor',\n",
       " 'big data set',\n",
       " 'big data set',\n",
       " 'big data set',\n",
       " 'per sentiment day',\n",
       " 'great medium article',\n",
       " 'small data set',\n",
       " 'av testing approach',\n",
       " 'november collecting articles',\n",
       " 'partial outcome condition',\n",
       " 'partial outcome condition',\n",
       " 'little bit modeling',\n",
       " 'little bit modeling',\n",
       " '44 years old',\n",
       " 'share room type',\n",
       " 'average price rating',\n",
       " 'every three months',\n",
       " 'surprisingly motor bikes',\n",
       " 'significant underper presentation',\n",
       " 'roll state investor',\n",
       " 'partner people prefer',\n",
       " 'partner people prefer',\n",
       " 'higher still ratio',\n",
       " 'good insightful conclusions',\n",
       " 'would perhaps like',\n",
       " 'feel less successful',\n",
       " 'online consumer behavior',\n",
       " 'quite high salaries',\n",
       " 'fundamental analysis measures',\n",
       " 'analysis view refreshes',\n",
       " 'like super pan',\n",
       " 'stack overflow always',\n",
       " '125 people answered',\n",
       " 'trial 50 something',\n",
       " 'like finding signals',\n",
       " 'really sad story',\n",
       " 'never fully control',\n",
       " 'pretty pretty amazing',\n",
       " 'little bit different',\n",
       " 'exploratory data analysis',\n",
       " 'exploratory data analysis',\n",
       " 'traffic accident report',\n",
       " 'like bullet points',\n",
       " 'big million effect',\n",
       " 'two different groups',\n",
       " 'minus 100 score',\n",
       " 'like dashboard feature',\n",
       " 'would strongly suggest',\n",
       " 'negative feedback loop',\n",
       " 'provide dedicated areas',\n",
       " 'boot camp',\n",
       " 'history behind melinda',\n",
       " 'pricing factor location',\n",
       " 'little visual cues',\n",
       " 'age range groups',\n",
       " 'entire apartment',\n",
       " 'buy 11 contracts',\n",
       " 'single people prefer',\n",
       " 'use another decay',\n",
       " 'collected active replies',\n",
       " 'use ab testing',\n",
       " 'data set manually',\n",
       " 'super good use',\n",
       " 'well structured test',\n",
       " 'avoid car accidents',\n",
       " 'multiple data sets',\n",
       " 'considering professional success',\n",
       " 'like super clear',\n",
       " 'rather scientific topic',\n",
       " 'new adrenaline effects',\n",
       " 'really enjoyed watching',\n",
       " 'one share room',\n",
       " 'like actual clue',\n",
       " 'difference engenders population',\n",
       " 'second study worldwide',\n",
       " 'man reasonable comment',\n",
       " 'directly super low',\n",
       " 'presentation perfectly great',\n",
       " 'impressive big collection',\n",
       " 'car traffic accidents',\n",
       " 'also clear tendency',\n",
       " 'means low budget',\n",
       " 'negative worded article',\n",
       " 'new spanish law',\n",
       " 'highest salary range',\n",
       " 'full outcome condition',\n",
       " 'full outcome condition',\n",
       " 'super important always',\n",
       " 'top three dashboard',\n",
       " 'marketing strategy later',\n",
       " 'one quick idea',\n",
       " 'giving p values',\n",
       " 'earn higher salaries',\n",
       " 'district average age',\n",
       " 'one indian artist',\n",
       " 'like gigantic improvement',\n",
       " 'filter explicitly like',\n",
       " 'pretty good still',\n",
       " 'one plus one',\n",
       " 'really dive dive',\n",
       " 'really useful way',\n",
       " 'pretty cool presentation',\n",
       " 'spend exercising increased',\n",
       " 'less price also',\n",
       " 'people would actually want',\n",
       " 'positive worded article',\n",
       " 'sensitivity analysis part',\n",
       " 'company values followed',\n",
       " 'something called django',\n",
       " 'data set first',\n",
       " 'little bit like',\n",
       " 'little bit like',\n",
       " 'pretty technical analysis',\n",
       " 'story driven presentation',\n",
       " 'clear control group',\n",
       " 'actually pretty impressive',\n",
       " 'like really impressive',\n",
       " 'choose two movies',\n",
       " 'also implemented already',\n",
       " 'different shapes',\n",
       " 'robust enough model',\n",
       " 'also technical data',\n",
       " 'powerful tools learned',\n",
       " 'really great idea',\n",
       " '10 answered yes',\n",
       " 'model fit would',\n",
       " 'course individual aspects',\n",
       " 'contingency test result',\n",
       " 'year 2021 december',\n",
       " 'really good idea',\n",
       " 'many children often',\n",
       " 'new market needs',\n",
       " 'least one situation',\n",
       " 'movie industry great',\n",
       " 'two days one',\n",
       " '3 minus 0',\n",
       " 'two different categories',\n",
       " 'really impressive efforts',\n",
       " 'often women feel',\n",
       " 'another good reason',\n",
       " 'spend longer time',\n",
       " 'pretty good insight',\n",
       " 'pretty great use',\n",
       " 'super interesting topic',\n",
       " 'drives people',\n",
       " 'test also turn',\n",
       " 'certain neurotransmitters respond',\n",
       " 'recommend like based',\n",
       " 'make back together',\n",
       " 'run several tests',\n",
       " 'great next step',\n",
       " 'properly like understand',\n",
       " 'news articles related',\n",
       " 'participants makes mistakes',\n",
       " 'less 80 responses',\n",
       " 'total 40 participants',\n",
       " 'bigger one let',\n",
       " 'top 100 artists',\n",
       " 'cinema chair want',\n",
       " 'stock market total',\n",
       " 'interactive like tableau',\n",
       " 'bullet points get',\n",
       " 'much time people said',\n",
       " 'collect recent data',\n",
       " 'also really cool',\n",
       " 'get better commissions',\n",
       " 'also quite impressive',\n",
       " 'must pay attention',\n",
       " 'really cool presentation',\n",
       " 'many bar charts',\n",
       " 'like seeing like',\n",
       " 'population mainly affected',\n",
       " 'good morning everyone',\n",
       " 'good morning everyone',\n",
       " 'good data analysis',\n",
       " 'traffic accident based',\n",
       " 'use night base',\n",
       " 'use chat js',\n",
       " 'make best price',\n",
       " 'technical aspects whenever',\n",
       " 'previous dashboard like',\n",
       " 'opposite hypothesis testing',\n",
       " 'great analytical project',\n",
       " 'movie studios also',\n",
       " 'giving us like',\n",
       " 'still experience accidents',\n",
       " 'also drop tiktok',\n",
       " 'found like creating',\n",
       " 'like technical work',\n",
       " 'analytical layer call',\n",
       " 'really specific amount',\n",
       " 'comes along life',\n",
       " 'see titanic movie',\n",
       " 'receive negative consequences',\n",
       " 'single artists females',\n",
       " 'public worker right',\n",
       " 'obviously losing lives',\n",
       " 'multi lines',\n",
       " 'group b',\n",
       " '56 trials',\n",
       " 'make good movies',\n",
       " 'make good movies',\n",
       " 'market analysis based',\n",
       " 'dashboard looks amazing',\n",
       " 'really clean presentation',\n",
       " 'like two things',\n",
       " 'actually spend exercising',\n",
       " 'consider professional success',\n",
       " 'general perception perception',\n",
       " 'yeah perfection exactly',\n",
       " 'front end stuff',\n",
       " 'location rating accommodation',\n",
       " 'brings higher revenues',\n",
       " 'details analysis part',\n",
       " 'also know tableau kind',\n",
       " 'negative articles release',\n",
       " 'draw solid conclusions',\n",
       " 'time online spend',\n",
       " 'midwood come project',\n",
       " 'always one symbol',\n",
       " '10 minutes',\n",
       " 'solid project overall',\n",
       " 'really impressive project',\n",
       " 'answers regarding studies',\n",
       " 'apartment type',\n",
       " 'single open question',\n",
       " 'would also like',\n",
       " 'also would like',\n",
       " 'people would prefer',\n",
       " 'sweet spot right',\n",
       " 'let participants participate',\n",
       " 'like really congratulations',\n",
       " 'google forms anymore',\n",
       " 'per se',\n",
       " 'use something running',\n",
       " 'positive negative correlation',\n",
       " 'highest salaries wanted',\n",
       " 'move industry great',\n",
       " 'really nice solution',\n",
       " 'like interesting enough',\n",
       " 'sample effect exists',\n",
       " 'always told like',\n",
       " 'important one followed',\n",
       " 'entire home',\n",
       " 'share something like',\n",
       " 'people really aim',\n",
       " 'really important part',\n",
       " 'apply many things',\n",
       " 'apply many things',\n",
       " 'also another category',\n",
       " 'analyzing anything technical',\n",
       " 'main differences',\n",
       " 'heard last week',\n",
       " 'average ticket',\n",
       " 'movie could bring',\n",
       " 'could better choose',\n",
       " 'revenue budget',\n",
       " 'get higher rewards',\n",
       " 'follow back casting',\n",
       " 'moving much closer',\n",
       " 'slide really well',\n",
       " 'people suspect anyway',\n",
       " 'general perception location',\n",
       " 'something similar happens',\n",
       " 'great analysis analysis',\n",
       " 'professional success mean',\n",
       " 'thousand dollars',\n",
       " 'normally says',\n",
       " 'survey also fun',\n",
       " 'see like many',\n",
       " 'per trial',\n",
       " 'learn another hypothesis',\n",
       " 'also showing like',\n",
       " 'goes really close',\n",
       " 'bar chart also',\n",
       " 'quick break break',\n",
       " 'get best return',\n",
       " 'help make like',\n",
       " 'queen next position',\n",
       " 'back end part',\n",
       " 'people would expect',\n",
       " 'stop participant sharing',\n",
       " 'python package used',\n",
       " 'awesome presentations next',\n",
       " 'like next projects',\n",
       " 'effort could make',\n",
       " 'really show like',\n",
       " 'maybe could affect',\n",
       " 'speed limits',\n",
       " 'speed limits',\n",
       " 'speed limits',\n",
       " 'speed limits',\n",
       " 'speed limits',\n",
       " 'speed limits',\n",
       " 'one else',\n",
       " 'weather condition',\n",
       " 'slot machine',\n",
       " 'power bi',\n",
       " 'conversion rate',\n",
       " 'main hypothesis',\n",
       " 'mean salingric level',\n",
       " 'mean salingric level',\n",
       " 'necessarily make movies',\n",
       " 'women prefer environment',\n",
       " 'save people belonging',\n",
       " 'build something visual',\n",
       " 'analysis specifically actually',\n",
       " 'main target',\n",
       " 'every month',\n",
       " 'stuff like yeah',\n",
       " 'speed limit',\n",
       " 'speed limit',\n",
       " 'speed limit',\n",
       " 'speed limit',\n",
       " 'create three profiles',\n",
       " 'would always tell',\n",
       " 'like taking care',\n",
       " 'would maybe content',\n",
       " 'would say like',\n",
       " 'would say like',\n",
       " 'informative next time',\n",
       " 'like facebook advertisement',\n",
       " 'like click',\n",
       " 'cinema chain',\n",
       " '25 dollar',\n",
       " 'stock goes low',\n",
       " 'user would want',\n",
       " 'good means 4',\n",
       " 'average price',\n",
       " '10 years',\n",
       " '10 years',\n",
       " 'three weeks',\n",
       " 'room type',\n",
       " 'would say target',\n",
       " 'strong modeling',\n",
       " 'learning condition',\n",
       " 'also think like',\n",
       " 'randomly take one',\n",
       " 'electric shocks',\n",
       " 'create really nice',\n",
       " 'airbnb listing',\n",
       " 'found something interesting',\n",
       " 'main dashboard',\n",
       " 'strong factor',\n",
       " 'strong factor',\n",
       " 'human factor',\n",
       " 'often drop',\n",
       " 'average casting',\n",
       " 'super impressive',\n",
       " 'less minutes',\n",
       " 'get things back',\n",
       " 'learned 19 questions',\n",
       " 'main topic',\n",
       " 'ab testing',\n",
       " 'main analysis',\n",
       " 'organic sales',\n",
       " 'week something whatever',\n",
       " 'reinforcement learning',\n",
       " 'reinforcement learning',\n",
       " 'reinforcement learning',\n",
       " 'reinforcement learning',\n",
       " 'main question',\n",
       " 'neighborhood groups',\n",
       " 'many hours',\n",
       " 'everything makes sense',\n",
       " 'little room',\n",
       " 'show one plot',\n",
       " 'email marketing',\n",
       " 'super cool',\n",
       " 'super cool',\n",
       " 'super cool',\n",
       " 'use one sample',\n",
       " 'total cost',\n",
       " 'technical components',\n",
       " 'technical components',\n",
       " 'main general',\n",
       " 'got one question',\n",
       " 'vegas nerve',\n",
       " 'stack overflow',\n",
       " 'motor bikes',\n",
       " 'missing operating',\n",
       " 'group activities',\n",
       " 'favorite actress',\n",
       " 'family reconciliation',\n",
       " 'energy activation',\n",
       " '350 euro',\n",
       " 'biggest driven',\n",
       " 'biggest driven',\n",
       " '50 answers',\n",
       " 'contingency test',\n",
       " 'contingency test',\n",
       " 'successful professional',\n",
       " 'successful professional',\n",
       " 'successful professional',\n",
       " 'main interest',\n",
       " 'took casting',\n",
       " 'digital nomads',\n",
       " 'control condition',\n",
       " 'pricing factor',\n",
       " 'pricing factor',\n",
       " 'pricing factor',\n",
       " 'several areas',\n",
       " 'successful professionals',\n",
       " 'successful professionals',\n",
       " 'successful professionals',\n",
       " 'strong correlation',\n",
       " 'strong data',\n",
       " 'say gonzalo saw',\n",
       " 'road condition',\n",
       " 'storytelling perspective',\n",
       " 'super islands',\n",
       " 'super helpful',\n",
       " 'super fast',\n",
       " 'super active',\n",
       " 'large one',\n",
       " 'condition one',\n",
       " 'urban areas',\n",
       " 'old young',\n",
       " 'average salary',\n",
       " 'two group',\n",
       " 'new incomes',\n",
       " 'example david gave',\n",
       " 'actual data',\n",
       " 'actual data',\n",
       " 'new stuff',\n",
       " 'modeling perception',\n",
       " 'deloitte single',\n",
       " 'private room',\n",
       " 'private room',\n",
       " 'private room',\n",
       " 'coming back',\n",
       " 'parents took',\n",
       " 'four blocks',\n",
       " 'annual incomes',\n",
       " '65 plus',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'current speed',\n",
       " 'stuff together',\n",
       " 'recent correlation',\n",
       " 'whole paper',\n",
       " 'investment dates',\n",
       " 'every purchase',\n",
       " 'age range',\n",
       " 'age range',\n",
       " 'age range',\n",
       " 'car accidents',\n",
       " 'data cleaning',\n",
       " 'data cleaning',\n",
       " 'data cleaning',\n",
       " 'data cleaning',\n",
       " 'technical goals',\n",
       " 'pretty good',\n",
       " 'pretty good',\n",
       " 'pretty good',\n",
       " 'people make right',\n",
       " 'weather conditions',\n",
       " 'weather conditions',\n",
       " 'weather conditions',\n",
       " 'super low',\n",
       " 'original study',\n",
       " 'strong indicator',\n",
       " 'sham condition',\n",
       " 'super interesting',\n",
       " 'investment day',\n",
       " 'super powerful',\n",
       " 'new hypothesis',\n",
       " 'new hypothesis',\n",
       " 'new hypothesis',\n",
       " 'new hypothesis',\n",
       " 'significant 11',\n",
       " 'super clear',\n",
       " 'marketing strategy',\n",
       " 'marketing strategy',\n",
       " 'marketing strategy',\n",
       " 'specific person',\n",
       " 'every presentation',\n",
       " 'technical stuff',\n",
       " 'p values',\n",
       " 'word like',\n",
       " 'like fun',\n",
       " 'like creating',\n",
       " 'several panels',\n",
       " 'least giving',\n",
       " 'new entry',\n",
       " 'anyone wants',\n",
       " 'anyone wants',\n",
       " 'anyone wants',\n",
       " 'anyone wants',\n",
       " 'anyone wants',\n",
       " 'different movies',\n",
       " 'road accidents',\n",
       " 'big road',\n",
       " 'data gathering',\n",
       " 'data gathering',\n",
       " 'strongest factor',\n",
       " 'prime factor',\n",
       " 'analytical layer',\n",
       " 'additional factor',\n",
       " 'almost every',\n",
       " 'almost every',\n",
       " 'professional development',\n",
       " 'hypothesis testing',\n",
       " 'hypothesis testing',\n",
       " 'hypothesis testing',\n",
       " 'hypothesis testing',\n",
       " 'hypothesis testing',\n",
       " 'hypothesis testing',\n",
       " 'share room',\n",
       " 'share room',\n",
       " 'share room',\n",
       " 'super curious',\n",
       " 'whole city',\n",
       " 'whole city',\n",
       " 'barcelona month',\n",
       " 'first month',\n",
       " 'two years',\n",
       " 'showcase one',\n",
       " 'heard one',\n",
       " 'year 2019',\n",
       " 'wrapped together',\n",
       " 'visual cues',\n",
       " 'unbending task',\n",
       " 'stumbling block',\n",
       " 'spanish company',\n",
       " 'small person',\n",
       " 'small company',\n",
       " 'shipping cost',\n",
       " 'quite simple',\n",
       " 'proton type',\n",
       " 'oldest group',\n",
       " 'night base',\n",
       " 'multiple variables',\n",
       " 'motivation behind',\n",
       " 'landing perfectly',\n",
       " 'house type',\n",
       " 'group risks',\n",
       " 'group age',\n",
       " 'electrical signals',\n",
       " 'election worker',\n",
       " 'driving license',\n",
       " 'cost replace',\n",
       " 'confidence interval',\n",
       " 'age group',\n",
       " 'age group',\n",
       " 'age group',\n",
       " '448 rows',\n",
       " '30 p',\n",
       " 'super congratulations',\n",
       " 'best accommodation',\n",
       " 'different locations',\n",
       " 'actual presentation',\n",
       " 'single people',\n",
       " 'technical like',\n",
       " 'looking forward',\n",
       " 'fundamental data',\n",
       " 'testing independence',\n",
       " 'selling high',\n",
       " 'car like',\n",
       " 'professional success',\n",
       " 'professional success',\n",
       " 'professional success',\n",
       " 'professional success',\n",
       " 'professional success',\n",
       " 'pretty neat',\n",
       " 'pretty flawed',\n",
       " 'pretty bad',\n",
       " 'computer test',\n",
       " 'specific store',\n",
       " 'suspect something',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'like collecting',\n",
       " 'certain groups',\n",
       " 'really interactive',\n",
       " 'really interactive',\n",
       " 'many staff',\n",
       " 'many others',\n",
       " 'many opportunities',\n",
       " 'many clicks',\n",
       " 'something different',\n",
       " 'five days',\n",
       " 'total sales',\n",
       " 'two groups',\n",
       " 'two groups',\n",
       " 'two groups',\n",
       " 'two groups',\n",
       " 'revenue development',\n",
       " 'quite successful',\n",
       " 'lead us',\n",
       " 'either doctors',\n",
       " '24 layers',\n",
       " 'also provide',\n",
       " 'ppt presentation',\n",
       " 'group presentation',\n",
       " 'two movies',\n",
       " 'market strategy',\n",
       " 'one data',\n",
       " 'every song',\n",
       " 'specific day',\n",
       " 'different language',\n",
       " 'technical teams',\n",
       " 'partial outcome',\n",
       " 'every time',\n",
       " 'every time',\n",
       " 'prior experience',\n",
       " 'less switching',\n",
       " 'less precise',\n",
       " 'graphic accidents',\n",
       " 'big players',\n",
       " 'big paragraphs',\n",
       " 'big intersections',\n",
       " 'big intersections',\n",
       " 'big improvement',\n",
       " '17 accidents',\n",
       " 'conditions together',\n",
       " 'good students',\n",
       " 'good students',\n",
       " 'help set',\n",
       " 'help set',\n",
       " 'really dive',\n",
       " 'zero points',\n",
       " 'spanish government',\n",
       " 'small application',\n",
       " 'quick summary',\n",
       " 'quick inter',\n",
       " 'neurological responses',\n",
       " 'minor differences',\n",
       " 'last slide',\n",
       " 'last slide',\n",
       " 'dedicated areas',\n",
       " 'december 12th',\n",
       " 'came 930',\n",
       " '62 dollars',\n",
       " '6 dollars',\n",
       " 'tech',\n",
       " 'ink ratio',\n",
       " 'really good',\n",
       " 'really good',\n",
       " 'car accident',\n",
       " 'car accident',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "+# Keywords\n",
    "\n",
    "#RAKE\n",
    "from rake_nltk import Rake\n",
    "rake = Rake()\n",
    "rake.extract_keywords_from_text(result['text'])\n",
    "extracted_keyword = rake.get_ranked_phrases()\n",
    "extracted_keyword"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c2f1846f3955305d6cdcd7be5897be31bd89b0ce061940c94fabaf8ec721e2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
