{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtextwrap\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m#Pandas\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/pandas/__init__.py:138\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m    121\u001b[0m     concat,\n\u001b[1;32m    122\u001b[0m     lreshape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     qcut,\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[0;32m--> 138\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m testing  \u001b[39m# noqa:PDF015\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_print_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[1;32m    141\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m    142\u001b[0m     \u001b[39m# excel\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     ExcelFile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m     read_spss,\n\u001b[1;32m    172\u001b[0m )\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/pandas/testing.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mPublic testing utility functions.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_testing\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     assert_extension_array_equal,\n\u001b[1;32m      8\u001b[0m     assert_frame_equal,\n\u001b[1;32m      9\u001b[0m     assert_index_equal,\n\u001b[1;32m     10\u001b[0m     assert_series_equal,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_extension_array_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_frame_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_series_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39massert_index_equal\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m ]\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/pandas/_testing/__init__.py:903\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpytest\u001b[39;00m\n\u001b[1;32m    900\u001b[0m     \u001b[39mreturn\u001b[39;00m pytest\u001b[39m.\u001b[39mraises(expected_exception, match\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)  \u001b[39m# noqa: PDF010\u001b[39;00m\n\u001b[0;32m--> 903\u001b[0m cython_table \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mcore\u001b[39m.\u001b[39mcommon\u001b[39m.\u001b[39m_cython_table\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    906\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cython_table_params\u001b[39m(ndframe, func_names_and_expected):\n\u001b[1;32m    907\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[39m    Combine frame, functions from com._cython_table\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[39m    keys and expected result.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39m        List of three items (DataFrame, function, expected result)\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "\n",
    "#Connect to mysql\n",
    "import mysql.connector\n",
    "\n",
    "#Operating system\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from datetime import timedelta\n",
    "\n",
    "#Transcription & subtitles\n",
    "import whisper\n",
    "import stable_whisper\n",
    "\n",
    "#Translation\n",
    "import translators as ts\n",
    "import translators.server as tss\n",
    "import textwrap\n",
    "\n",
    "#Pandas\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(filepath):\n",
    "    \"\"\"\n",
    "    With this function we can transcribe all the texts from a video/audio and also the subtitles\n",
    "    \n",
    "    Keyword arguments:\n",
    "    argument -- filepath:the file to transcribe\n",
    "    Return: No return (insert in mysql and create 2 text files -temporary-)\n",
    "    \"\"\"\n",
    "    \n",
    "    video = os.path.split(os.path.abspath(Path(filepath)))\n",
    "    name = video[1].split(sep='.')\n",
    "\n",
    "    result_sql = insert_data_sql('video', '', '', filepath)\n",
    "    videoid = result_sql.lastrowid\n",
    "    \n",
    "    # speech transcription\n",
    "    \n",
    "    model = whisper.load_model(\"base.en\",device='cpu')\n",
    "    #model = stable_whisper.load_model('base')\n",
    "    result = model.transcribe(filepath)\n",
    "    \n",
    "    with open(Path(video[0]+\"/\"+name[0]+\"_transcription.txt\"), \"w+\") as f:\n",
    "        f.write(result[\"text\"])\n",
    "\n",
    "    result_sql = insert_data_sql('transcription', videoid, 'en', result['text'])\n",
    "\n",
    "\n",
    "    # subtitles \n",
    "    stable_whisper.results_to_sentence_srt(result, video[0]+\"/\"+name[0]+\"_subtitles.srt\")\n",
    "\n",
    "    text_subtitles = open(video[0]+\"/\"+name[0]+\"_subtitles.srt\").read()\n",
    "\n",
    "    result_sql = insert_data_sql('subtitle', videoid, 'en', text_subtitles)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transcribe('/home/roque/01. IronHack/00. Data Analytics/01. Course/04. Week 2 - Day 1/recordings/GMT20220913-171210_Recording_1920x1080.mp4')\n",
    "#transcribe('/home/roque/01. IronHack/00. Data Analytics/01. Course/02. Week 1 - Day 2/Recordings/GMT20220908-163034_Recording_1920x1080.mp4')\n",
    "#transcribe('/home/roque/01. IronHack/00. Data Analytics/01. Course/01. Week 1 - Day 3/Recordings/GMT20220910-082751_Recording_1920x1080.mp4')\n",
    "#transcribe('/home/roque/01. IronHack/00. Data Analytics/01. Course/01. Week 2 - Day 1/Recordings/GMT20220913-171210_Recording_1920x1080.mp4')\n",
    "#transcribe('/home/roque/01. IronHack/00. Data Analytics/01. Course/01. Week 2 - Day 2/Recordings/GMT20220915-163321_Recording_1920x1080.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the subtitles, can use: vlc test.mp4 --sub-file test2.srt\n",
    "#subprocess.call(['vlc', '/home/roque/01. IronHack/00. Data Analytics/01. Course/03. Week 1 - Day 3/recordings/GMT20220910-082751_Recording_1920x1080.mp4', '--sub-file', '/home/roque/01. IronHack/00. Data Analytics/01. Course/03. Week 1 - Day 3/recordings/GMT20220910-082751_Recording_1920x1080_subtitles.srt', '--start-time', '3120'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_player(_videoid, _langid, _position = 0):\n",
    "    conn = connection_ddbb()\n",
    "    cursor = conn.cursor() \n",
    "\n",
    "    if conn.is_connected:\n",
    "        #Configuration table\n",
    "        query = \"\"\"SELECT video_player, temp_directory\n",
    "                     FROM ironrep.configuration \n",
    "                     LIMIT 1;\"\"\"\n",
    "\n",
    "        cursor.execute(query)\n",
    "        conf_table = cursor.fetchall()\n",
    "        conf_df = pd.DataFrame(conf_table)\n",
    "        conf_df.columns = [i[0] for i in cursor.description]\n",
    "\n",
    "        #subtitles\n",
    "\n",
    "        #first read the subtitles from the database\n",
    "        query = \"\"\"SELECT subtitles\n",
    "                     FROM ironrep.subtitles\"\"\"\n",
    "        cursor.execute(query)\n",
    "        subt_table = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m cursor\u001b[39m.\u001b[39mexecute(query)\n\u001b[1;32m      8\u001b[0m conf_table \u001b[39m=\u001b[39m cursor\u001b[39m.\u001b[39mfetchall()\n\u001b[0;32m----> 9\u001b[0m conf_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(conf_table)\n\u001b[1;32m     10\u001b[0m conf_df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [i[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m cursor\u001b[39m.\u001b[39mdescription]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "conn = connection_ddbb()\n",
    "cursor = conn.cursor() \n",
    "query = \"\"\"SELECT video_player, temp_directory\n",
    "                     FROM ironrep.configuration \n",
    "                     LIMIT 1;\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "conf_table = cursor.fetchall()\n",
    "conf_df = pd.DataFrame(conf_table)\n",
    "conf_df.columns = [i[0] for i in cursor.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vlc {videoparam} --sub-file {subtitleparam} --start-time {positionparam}',\n",
       "  '/home/roque/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/data/')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_from_en(text_en,to_language):\n",
    "    \"\"\"FUNCTION TO TRANSLATE THE TEXT FROM ENGLISH TO ANY LANGUAGE\n",
    "    \n",
    "    Keyword arguments:\n",
    "    text_en: text in english\n",
    "    to_language: the code of the language to translate (es,zh, ...) \n",
    "    Return: text translated\n",
    "    \"\"\"\n",
    "    \n",
    "    if (to_language==''):\n",
    "        display('Need to include the language to translate.')\n",
    "    else:\n",
    "        ts.translators_pool\n",
    "\n",
    "        text_translated = ''\n",
    "        \n",
    "        from_language = 'en'\n",
    "            \n",
    "        \n",
    "        if (len(text_en)>2000):\n",
    "            textsplited_to = []\n",
    "            text_splited = textwrap.wrap(text_en, 2000, break_long_words=False)\n",
    "            for line in text_splited:\n",
    "                textsplited_to.append(tss.google(line, from_language, to_language))\n",
    "            text_translated = ' '.join(textsplited_to)\n",
    "        else:\n",
    "            try:\n",
    "                text_translated = tss.google(text_en, from_language, to_language)\n",
    "            except:\n",
    "                try:\n",
    "                    text_translated = ts.translate_text(text_en, from_language, to_language)\n",
    "                except:\n",
    "                    text_translated = text_en\n",
    "\n",
    "    return text_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_from_en('hello','zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_ddbb():\n",
    "    \"\"\"Function to create the conection to the data base\n",
    "    \n",
    "    Keyword arguments:    \n",
    "    Return: connection objet\n",
    "    \"\"\"\n",
    "       \n",
    "\n",
    "    secrets={}\n",
    "    #secrets_file = open('secrets.txt','r') #Had errors using this within a virtual environment\n",
    "    secrets_file = os.fdopen(os.open('secrets.txt', os.O_RDONLY))\n",
    "    for line in secrets_file:\n",
    "        (key, val) = line.replace('\\n','').split(\"|\")\n",
    "        secrets[key] = val    \n",
    "\n",
    "    #Conection to mysql\n",
    "\n",
    "    conn = mysql.connector.connect(user=secrets['user'],\n",
    "                            password=secrets['pass'],\n",
    "                            host=secrets['server'])\n",
    "    \n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ddbb():\n",
    "    \"\"\"Function to create the data base\n",
    "    \n",
    "    Keyword arguments:    \n",
    "    Return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = connection_ddbb()\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    #Creating schema in mysql\n",
    "\n",
    "    if conn.is_connected():\n",
    "        print('Connection open')        \n",
    "        \n",
    "        print('Creating database if necessary...')\n",
    "        \n",
    "        query = ('CREATE DATABASE IF NOT EXISTS ironrep')\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "        query = ('USE ironrep')\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "        print('Database created if necessary...')\n",
    "\n",
    "        print('Creating tables if necessary...')\n",
    "\n",
    "        #Configuration        \n",
    "        \n",
    "        query = (\"\"\"CREATE TABLE IF NOT EXISTS ironrep.configuration (  \n",
    "                        id enum('1') PRIMARY KEY NOT NULL,\n",
    "                        temp_directory  nvarchar(250),\n",
    "                        video_player nvarchar(250),\n",
    "                        languages_subtitles nvarchar(250) COMMENT 'List of langages codes separated by commas')\"\"\")\n",
    "\n",
    "        cursor.execute(query)    \n",
    "        conn.commit()\n",
    "\n",
    "        #Default values\n",
    "        query = (\"\"\"REPLACE INTO ironrep.configuration (temp_directory,video_player,languages_subtitles)\n",
    "                        VALUES (%s,%s,%s)\"\"\")\n",
    "        val = (str(secrets['temp_dir']),str(secrets['video_play']),str(secrets['lang_subt']))\n",
    "        \"\"\"val = (str('/home/roque/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/data/'),\n",
    "                str('vlc {videoparam} --sub-file {subtitleparam} --start-time {positionparam}'),\n",
    "                str('es,pt,it,zh,de,hi'))\"\"\"\n",
    "        cursor.execute(query,val)\n",
    "        conn.commit()\n",
    "\n",
    "        #Videos\n",
    "\n",
    "        query = (\"\"\"CREATE TABLE IF NOT EXISTS ironrep.videos (\n",
    "                        id INT PRIMARY KEY NOT NULL AUTO_INCREMENT,\n",
    "                        video nvarchar(250),\n",
    "                        video_path nvarchar(250)\n",
    "                    )\"\"\"\n",
    "        )\n",
    "\n",
    "        cursor.execute(query)    \n",
    "        conn.commit()\n",
    "\n",
    "        #Transcriptions\n",
    "\n",
    "        query = (\"\"\"CREATE TABLE IF NOT EXISTS ironrep.transcriptions (\n",
    "                        id INT PRIMARY KEY NOT NULL AUTO_INCREMENT,\n",
    "                        videoid INT NOT NULL,\n",
    "                        languageid nvarchar(5),\n",
    "                        transcription mediumtext\n",
    "                    )\"\"\"\n",
    "        )\n",
    "\n",
    "        cursor.execute(query)    \n",
    "        conn.commit()\n",
    "\n",
    "        #Summaries\n",
    "\n",
    "        query = (\"\"\"CREATE TABLE IF NOT EXISTS ironrep.summaries (\n",
    "                        videoid INT NOT NULL,\n",
    "                        languageid nvarchar(5),\n",
    "                        summary nvarchar(260)                   \n",
    "                    )\"\"\"\n",
    "        )\n",
    "\n",
    "        cursor.execute(query)    \n",
    "        conn.commit()\n",
    "\n",
    "        #Subtitles\n",
    "\n",
    "        query = (\"\"\"CREATE TABLE IF NOT EXISTS ironrep.subtitles (\n",
    "                        id INT PRIMARY KEY NOT NULL AUTO_INCREMENT,\n",
    "                        videoid INT NOT NULL,\n",
    "                        languageid nvarchar(5),\n",
    "                        subtitles mediumtext                   \n",
    "                    )\"\"\"\n",
    "        )\n",
    "\n",
    "        cursor.execute(query)    \n",
    "        conn.commit()\n",
    "\n",
    "        #Keywords\n",
    "\n",
    "        query = (\"\"\"CREATE TABLE IF NOT EXISTS ironrep.keywords (\n",
    "                        id INT PRIMARY KEY NOT NULL AUTO_INCREMENT,\n",
    "                        videoid INT NOT NULL,\n",
    "                        languageid nvarchar(5),\n",
    "                        keywords nvarchar(250)                   \n",
    "                    )\"\"\"\n",
    "        )\n",
    "\n",
    "        cursor.execute(query)    \n",
    "        conn.commit()\n",
    "\n",
    "        print('Tables created if necessary...')\n",
    "    else:\n",
    "        print('Error connecting')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_sql(_table, _videoid, _langid, _text):\n",
    "    \"\"\"Function to insert data in the database\n",
    "    \n",
    "    Keyword arguments:\n",
    "    _table: the object where insert ['video','transcription','subtitle','summary','keywords']\n",
    "    _videoid: the id of the video\n",
    "    _langid: the id of the language\n",
    "    _text: value to insert\n",
    "    Return: cursor\n",
    "    \"\"\"\n",
    "    \n",
    "    #Connection to the ddbb\n",
    "\n",
    "    conn = connection_ddbb()\n",
    "\n",
    "    if conn.is_connected():\n",
    "        query = ''\n",
    "        val = []\n",
    "\n",
    "        if (_table == 'video'):                 \n",
    "            vid_path = os.path.split(os.path.abspath(Path(_text)))\n",
    "\n",
    "            query = \"\"\"INSERT INTO ironrep.videos(video,video_path)\n",
    "                        VALUES (%s,%s)\"\"\"\n",
    "            val = [vid_path[1].split('.')[0],_text]\n",
    "        elif (_table == 'transcription'):\n",
    "            query = \"\"\"INSERT INTO ironrep.transcriptions(videoid,languageid,transcription)\n",
    "                        VALUES (%s,%s,%s)\"\"\"\n",
    "            val = [int(_videoid),_langid,_text]\n",
    "        elif (_table == 'subtitle'):\n",
    "            query = \"\"\"INSERT INTO ironrep.subtitles(videoid,languageid,subtitles)\n",
    "                        VALUES (%s,%s,%s)\"\"\"\n",
    "            val = [int(_videoid),_langid,_text]\n",
    "        elif (_table == 'summary'):\n",
    "            query = \"\"\"INSERT INTO ironrep.summary(videoid,languageid,summary)\n",
    "                        VALUES (%s,%s,%s)\"\"\"\n",
    "            val = [int(_videoid),_langid,_text]\n",
    "        elif (_table == 'keywords'):\n",
    "            query = \"\"\"INSERT INTO ironrep.keywords(videoid,languageid,keywords)\n",
    "                        VALUES (%s,%s,%s)\"\"\"\n",
    "            val = [int(_videoid),_langid,_text]\n",
    "        else:\n",
    "            return 'none'\n",
    "\n",
    "        if (query != ''):      \n",
    "            cursor.execute(query,val)\n",
    "            conn.commit()  \n",
    "            return cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "# TODO: Try with a larger text\n",
    "\n",
    "# python -m spacy download en_core_web_sm #eficency\n",
    "# python -m spacy download en_core_web_trf #accuracy\n",
    "\n",
    "import spacy\n",
    "import en_core_web_trf\n",
    "#import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy_transformers\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "def summarize(text, per):\n",
    "    #nlp = spacy.load('en_core_web_trf')\n",
    "    nlp = en_core_web_trf.load()\n",
    "    doc= nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens= [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "    select_length=int(len(sentence_tokens)*per)\n",
    "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "    final_summary=[word.text for word in summary]\n",
    "    summary=''.join(final_summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(result['text'], 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like one main one main one main suggestion',\n",
       " 'whole thing like drives like applies force',\n",
       " '30 grammets per hour speed limits',\n",
       " 'insanely strong tech tech right behind',\n",
       " 'pretty high cpc cost per click',\n",
       " 'new race fund group b',\n",
       " 'analysis like drives towards towards',\n",
       " 'passengers involved involving car accident obviously',\n",
       " 'strong hypothesis like super well laid',\n",
       " 'getting like disparate data together like',\n",
       " 'regular two bars one one next',\n",
       " 'pretty intense machine learning technique',\n",
       " 'specific works like vhvac shit',\n",
       " 'thousand dollar investment per trade',\n",
       " 'barcelona like 50 speed limit pro',\n",
       " 'available like entire home apartment',\n",
       " 'data set time seven parameters',\n",
       " 'create something called budget persona',\n",
       " '46 per 46 values',\n",
       " 'transcutaneous vagus nerve stimulation',\n",
       " 'organic email marketing also influencer',\n",
       " 'entire home apartment type cost',\n",
       " 'think maybe 10 shapes something like',\n",
       " 'normally find hypothesis test hypothesis find',\n",
       " 'great like storytelling storytelling journey',\n",
       " 'counting 1003 accidents totally',\n",
       " 'korean korean blue band',\n",
       " '50 kilometers per hours',\n",
       " '1600 euro per month',\n",
       " 'higher price average listing price',\n",
       " 'like everything like every technical point',\n",
       " 'year bnb pricing factor dashboard',\n",
       " 'many data came much data came',\n",
       " 'complex midbook camp project',\n",
       " 'hang group equals false',\n",
       " 'serial dot zero five',\n",
       " 'successful professional life people could choose',\n",
       " 'seven point 88 total dollars',\n",
       " 'actual weather condition condition basically',\n",
       " 'neighborhood different average listings',\n",
       " 'progress takes place outside',\n",
       " 'pricing listing price also higher',\n",
       " 'mini boot camp project',\n",
       " 'variables like caffeine alcohol',\n",
       " 'one conversion one one sell',\n",
       " 'someone lost someone wins',\n",
       " 'new york city manhattan brooklyn',\n",
       " 'mid boot camp project',\n",
       " 'like normalization normalization work',\n",
       " 'room time room type also',\n",
       " 'different average pricing listing',\n",
       " 'specific called data dredging',\n",
       " 'congratulations folks anyone else wants',\n",
       " 'think like geospatial maps',\n",
       " 'one super small curiosity',\n",
       " 'second largest cinema chain',\n",
       " '56 decisions per block',\n",
       " 'took several pretty interesting conclusions',\n",
       " 'use kyke square testing',\n",
       " 'chosen three neighborhoods group',\n",
       " '30s rather small areas',\n",
       " 'natural language processing going',\n",
       " 'day mainly recording mainly accident',\n",
       " 'little bit behind schedule',\n",
       " 'nothing telling us either',\n",
       " 'average price per 90',\n",
       " 'manhattan average listing price',\n",
       " 'great great great great topic',\n",
       " 'statistical analysis like set hypothesis',\n",
       " 'inadequate information operating accommodation',\n",
       " 'final error five features',\n",
       " 'things like clinical trials',\n",
       " 'like every dashboard allows',\n",
       " 'super blocks planning basically',\n",
       " 'collected 93 data records',\n",
       " 'even though correlating everything',\n",
       " 'sham condition per day',\n",
       " 'someone receives positive consequences',\n",
       " 'like complex like everything',\n",
       " 'little technical components driving',\n",
       " 'three articles per day',\n",
       " 'speed limits data set',\n",
       " 'use like base theorem',\n",
       " 'top three cinema chain',\n",
       " 'overall data per day',\n",
       " 'used past market data',\n",
       " 'original big data set',\n",
       " 'two take home messages',\n",
       " 'much every click cost',\n",
       " 'regular physical activities',\n",
       " 'like average item price',\n",
       " 'multi lines probably road',\n",
       " 'power bi like like',\n",
       " 'user spend less energy',\n",
       " 'best compliment right like',\n",
       " 'nice 10 minutes exactly',\n",
       " 'actually pretty reasonable dashboard',\n",
       " 'actual road condition obviously',\n",
       " 'strong point influencing factor',\n",
       " 'clearly like two weeks',\n",
       " 'like super high marks',\n",
       " 'email influencer cause',\n",
       " 'first mentioned reinforcement learning',\n",
       " 'like super important coming',\n",
       " 'one group age range',\n",
       " 'thousand dollar investment',\n",
       " 'room type rating accommodation',\n",
       " 'available data sheet',\n",
       " 'entire home apartment',\n",
       " 'basically giving electric shocks',\n",
       " 'looks like super amazing',\n",
       " 'little bit data cleaning',\n",
       " 'little bit bullet point',\n",
       " 'even though average salary',\n",
       " 'profit revenue ratio basically',\n",
       " 'average listing price',\n",
       " 'unknown underlying mechanisms',\n",
       " 'mathematical computational models',\n",
       " 'administering atom moxetine',\n",
       " 'super professional looking dashboard',\n",
       " 'single day without articles',\n",
       " 'least one one step',\n",
       " 'work family consideration followed',\n",
       " 'highest average listing',\n",
       " 'spent per day',\n",
       " 'medium average ticket',\n",
       " 'bnb data set',\n",
       " 'least one presentation like',\n",
       " 'work coming back home',\n",
       " 'feel also male actress',\n",
       " 'every single square',\n",
       " 'less people feel successful',\n",
       " 'general perception pricing depends',\n",
       " 'two stimulation methods',\n",
       " 'chi square test',\n",
       " 'multi linear modeling',\n",
       " 'also like super nice',\n",
       " 'total cylindrical capacity',\n",
       " 'movie industry really hard',\n",
       " 'slightly behind schedule',\n",
       " 'prior bike handles',\n",
       " 'confident conference interval',\n",
       " 'particular type also like',\n",
       " 'different languages translated',\n",
       " 'looking forward equipment classes',\n",
       " 'successful professionals would make',\n",
       " 'higher rewards actually lead',\n",
       " 'basically totally sue',\n",
       " 'positive news articles released',\n",
       " 'investment per release',\n",
       " 'mid term presentation',\n",
       " 'market operating schedule',\n",
       " 'organic organic sales',\n",
       " 'location rating accommodation details',\n",
       " 'small average tickets',\n",
       " 'finding bright spots',\n",
       " 'clear hypothesis driven analysis',\n",
       " 'speed limits planning',\n",
       " 'seems like less content',\n",
       " 'find hypothesis test hypothesis',\n",
       " 'considering good students related',\n",
       " 'yet sufficiently practical',\n",
       " 'nurse whatsoever running',\n",
       " 'love new york',\n",
       " 'nice folks anyone wants',\n",
       " 'handmade jewelry online',\n",
       " 'high average ticket',\n",
       " 'super nice topic also',\n",
       " 'main data set',\n",
       " 'per word used',\n",
       " 'square contingency test',\n",
       " 'like mild severe',\n",
       " 'like default feedbacks',\n",
       " 'driving installation like',\n",
       " 'big broad range',\n",
       " 'charts actually like showcase',\n",
       " 'cause every month',\n",
       " 'super intelligent person',\n",
       " 'obviously materially impactful',\n",
       " 'essential design principles',\n",
       " 'different neighborhood group',\n",
       " 'different neighborhood group',\n",
       " 'used something called tv',\n",
       " 'post pandemic difficulties maybe',\n",
       " 'achieved educational level',\n",
       " 'know weather conditions based',\n",
       " 'something html css',\n",
       " 'digital nomads correlator',\n",
       " 'many different ages',\n",
       " 'extremely simple mind would',\n",
       " 'anyone else wants',\n",
       " 'chinese language chinese',\n",
       " 'high expectations lead',\n",
       " 'main like differences',\n",
       " '65 dollars theoretically',\n",
       " 'two neighborhood group',\n",
       " 'human factor cause',\n",
       " 'two different ways',\n",
       " 'two different blocks',\n",
       " 'need good data quality',\n",
       " 'every blue things',\n",
       " 'like strong notes',\n",
       " 'clear v shape',\n",
       " 'average annual incomes',\n",
       " 'smaller data set',\n",
       " 'limited data set',\n",
       " 'data cleaning translation',\n",
       " 'bullet point restating',\n",
       " 'different station life',\n",
       " 'specific human cause',\n",
       " 'used also positive correlation',\n",
       " 'average school grade',\n",
       " 'average school grade',\n",
       " 'news articles influence amazon',\n",
       " 'really congratulations like even',\n",
       " 'q square test',\n",
       " 'price average price',\n",
       " 'test one sided',\n",
       " 'different engenders population',\n",
       " 'roughly 55 minutes',\n",
       " 'account learning noise',\n",
       " 'robust predictive model',\n",
       " 'introduce technical data',\n",
       " 'finished fundamental studies',\n",
       " 'favorite actual actress',\n",
       " 'digital nomads longer',\n",
       " 'different reinforcement learning',\n",
       " 'less correlation among',\n",
       " 'new road planning',\n",
       " 'new calculated value',\n",
       " 'robust data gathering',\n",
       " '30 years old',\n",
       " 'whatsapp group chat',\n",
       " 'tightly connected across',\n",
       " 'technically complicated context',\n",
       " '30 second intervention',\n",
       " 'back million effects',\n",
       " 'spend every month',\n",
       " 'spend every month',\n",
       " 'like technical solidity',\n",
       " 'biggest driven year',\n",
       " 'subsequent professional success',\n",
       " 'whole data set',\n",
       " 'two ab testing',\n",
       " 'little bit stronger',\n",
       " 'little bit shape',\n",
       " 'little bit dying',\n",
       " 'power bi basically',\n",
       " 'think people feel good',\n",
       " '24 years old',\n",
       " '24 years old',\n",
       " '24 years old',\n",
       " 'recollecting 30 days',\n",
       " 'pc ppt point',\n",
       " 'email marketing strategy',\n",
       " 'large data set',\n",
       " 'large data set',\n",
       " 'bankruptcy list august',\n",
       " 'like urban planning',\n",
       " 'actual stimulation way',\n",
       " 'one sufficient data',\n",
       " 'interesting insights regarding gender',\n",
       " 'super block street',\n",
       " 'every single part',\n",
       " 'founded fantastic right',\n",
       " 'force like 4',\n",
       " 'title increased quite',\n",
       " 'achieve desired goals',\n",
       " 'top 10 songs',\n",
       " 'html stuff like',\n",
       " 'manhattan neighborhood group',\n",
       " 'two slides ago',\n",
       " 'online business idea',\n",
       " 'business online idea',\n",
       " 'task condition partial',\n",
       " 'yielding promising results',\n",
       " 'least five articles',\n",
       " 'several potential analysis',\n",
       " 'like correlates everything',\n",
       " 'new updated analysis',\n",
       " '10 minutes right',\n",
       " 'video stream revenues',\n",
       " 'last 20 %.',\n",
       " 'cinematographic industry grow',\n",
       " 'average ticket means',\n",
       " 'new societal change',\n",
       " 'quite advanced stuff',\n",
       " 'revenue dropped almost',\n",
       " 'bit like scared',\n",
       " 'cross relevant data',\n",
       " 'two percent presentation',\n",
       " 'also somewhat impressive',\n",
       " 'like really starving',\n",
       " 'less 70 rows',\n",
       " 'accidents took place',\n",
       " 'accidents took place',\n",
       " 'accidents took place',\n",
       " 'accidents took place',\n",
       " 'like super cool',\n",
       " 'less salary valued',\n",
       " 'box office front',\n",
       " 'trading either stocks',\n",
       " 'like next steps',\n",
       " 'many different stuff',\n",
       " 'top five movies',\n",
       " 'see like presentation thing',\n",
       " 'almost 10 years',\n",
       " 'pretty pretty solid',\n",
       " 'would good feet',\n",
       " 'hypothetical investment case',\n",
       " 'nice folks excellent presentation',\n",
       " 'granular details analysis',\n",
       " 'like alicia ed',\n",
       " 'html form like',\n",
       " 'getting like severely',\n",
       " 'also strong factor',\n",
       " 'big data set',\n",
       " 'big data set',\n",
       " 'big data set',\n",
       " 'per sentiment day',\n",
       " 'great medium article',\n",
       " 'small data set',\n",
       " 'av testing approach',\n",
       " 'november collecting articles',\n",
       " 'partial outcome condition',\n",
       " 'partial outcome condition',\n",
       " 'little bit modeling',\n",
       " 'little bit modeling',\n",
       " '44 years old',\n",
       " 'share room type',\n",
       " 'average price rating',\n",
       " 'every three months',\n",
       " 'surprisingly motor bikes',\n",
       " 'significant underper presentation',\n",
       " 'roll state investor',\n",
       " 'partner people prefer',\n",
       " 'partner people prefer',\n",
       " 'higher still ratio',\n",
       " 'good insightful conclusions',\n",
       " 'would perhaps like',\n",
       " 'feel less successful',\n",
       " 'online consumer behavior',\n",
       " 'quite high salaries',\n",
       " 'fundamental analysis measures',\n",
       " 'analysis view refreshes',\n",
       " 'like super pan',\n",
       " 'stack overflow always',\n",
       " '125 people answered',\n",
       " 'trial 50 something',\n",
       " 'like finding signals',\n",
       " 'really sad story',\n",
       " 'never fully control',\n",
       " 'pretty pretty amazing',\n",
       " 'little bit different',\n",
       " 'exploratory data analysis',\n",
       " 'exploratory data analysis',\n",
       " 'traffic accident report',\n",
       " 'like bullet points',\n",
       " 'big million effect',\n",
       " 'two different groups',\n",
       " 'minus 100 score',\n",
       " 'like dashboard feature',\n",
       " 'would strongly suggest',\n",
       " 'negative feedback loop',\n",
       " 'provide dedicated areas',\n",
       " 'boot camp',\n",
       " 'history behind melinda',\n",
       " 'pricing factor location',\n",
       " 'little visual cues',\n",
       " 'age range groups',\n",
       " 'entire apartment',\n",
       " 'buy 11 contracts',\n",
       " 'single people prefer',\n",
       " 'use another decay',\n",
       " 'collected active replies',\n",
       " 'use ab testing',\n",
       " 'data set manually',\n",
       " 'super good use',\n",
       " 'well structured test',\n",
       " 'avoid car accidents',\n",
       " 'multiple data sets',\n",
       " 'considering professional success',\n",
       " 'like super clear',\n",
       " 'rather scientific topic',\n",
       " 'new adrenaline effects',\n",
       " 'really enjoyed watching',\n",
       " 'one share room',\n",
       " 'like actual clue',\n",
       " 'difference engenders population',\n",
       " 'second study worldwide',\n",
       " 'man reasonable comment',\n",
       " 'directly super low',\n",
       " 'presentation perfectly great',\n",
       " 'impressive big collection',\n",
       " 'car traffic accidents',\n",
       " 'also clear tendency',\n",
       " 'means low budget',\n",
       " 'negative worded article',\n",
       " 'new spanish law',\n",
       " 'highest salary range',\n",
       " 'full outcome condition',\n",
       " 'full outcome condition',\n",
       " 'super important always',\n",
       " 'top three dashboard',\n",
       " 'marketing strategy later',\n",
       " 'one quick idea',\n",
       " 'giving p values',\n",
       " 'earn higher salaries',\n",
       " 'district average age',\n",
       " 'one indian artist',\n",
       " 'like gigantic improvement',\n",
       " 'filter explicitly like',\n",
       " 'pretty good still',\n",
       " 'one plus one',\n",
       " 'really dive dive',\n",
       " 'really useful way',\n",
       " 'pretty cool presentation',\n",
       " 'spend exercising increased',\n",
       " 'less price also',\n",
       " 'people would actually want',\n",
       " 'positive worded article',\n",
       " 'sensitivity analysis part',\n",
       " 'company values followed',\n",
       " 'something called django',\n",
       " 'data set first',\n",
       " 'little bit like',\n",
       " 'little bit like',\n",
       " 'pretty technical analysis',\n",
       " 'story driven presentation',\n",
       " 'clear control group',\n",
       " 'actually pretty impressive',\n",
       " 'like really impressive',\n",
       " 'choose two movies',\n",
       " 'also implemented already',\n",
       " 'different shapes',\n",
       " 'robust enough model',\n",
       " 'also technical data',\n",
       " 'powerful tools learned',\n",
       " 'really great idea',\n",
       " '10 answered yes',\n",
       " 'model fit would',\n",
       " 'course individual aspects',\n",
       " 'contingency test result',\n",
       " 'year 2021 december',\n",
       " 'really good idea',\n",
       " 'many children often',\n",
       " 'new market needs',\n",
       " 'least one situation',\n",
       " 'movie industry great',\n",
       " 'two days one',\n",
       " '3 minus 0',\n",
       " 'two different categories',\n",
       " 'really impressive efforts',\n",
       " 'often women feel',\n",
       " 'another good reason',\n",
       " 'spend longer time',\n",
       " 'pretty good insight',\n",
       " 'pretty great use',\n",
       " 'super interesting topic',\n",
       " 'drives people',\n",
       " 'test also turn',\n",
       " 'certain neurotransmitters respond',\n",
       " 'recommend like based',\n",
       " 'make back together',\n",
       " 'run several tests',\n",
       " 'great next step',\n",
       " 'properly like understand',\n",
       " 'news articles related',\n",
       " 'participants makes mistakes',\n",
       " 'less 80 responses',\n",
       " 'total 40 participants',\n",
       " 'bigger one let',\n",
       " 'top 100 artists',\n",
       " 'cinema chair want',\n",
       " 'stock market total',\n",
       " 'interactive like tableau',\n",
       " 'bullet points get',\n",
       " 'much time people said',\n",
       " 'collect recent data',\n",
       " 'also really cool',\n",
       " 'get better commissions',\n",
       " 'also quite impressive',\n",
       " 'must pay attention',\n",
       " 'really cool presentation',\n",
       " 'many bar charts',\n",
       " 'like seeing like',\n",
       " 'population mainly affected',\n",
       " 'good morning everyone',\n",
       " 'good morning everyone',\n",
       " 'good data analysis',\n",
       " 'traffic accident based',\n",
       " 'use night base',\n",
       " 'use chat js',\n",
       " 'make best price',\n",
       " 'technical aspects whenever',\n",
       " 'previous dashboard like',\n",
       " 'opposite hypothesis testing',\n",
       " 'great analytical project',\n",
       " 'movie studios also',\n",
       " 'giving us like',\n",
       " 'still experience accidents',\n",
       " 'also drop tiktok',\n",
       " 'found like creating',\n",
       " 'like technical work',\n",
       " 'analytical layer call',\n",
       " 'really specific amount',\n",
       " 'comes along life',\n",
       " 'see titanic movie',\n",
       " 'receive negative consequences',\n",
       " 'single artists females',\n",
       " 'public worker right',\n",
       " 'obviously losing lives',\n",
       " 'multi lines',\n",
       " 'group b',\n",
       " '56 trials',\n",
       " 'make good movies',\n",
       " 'make good movies',\n",
       " 'market analysis based',\n",
       " 'dashboard looks amazing',\n",
       " 'really clean presentation',\n",
       " 'like two things',\n",
       " 'actually spend exercising',\n",
       " 'consider professional success',\n",
       " 'general perception perception',\n",
       " 'yeah perfection exactly',\n",
       " 'front end stuff',\n",
       " 'location rating accommodation',\n",
       " 'brings higher revenues',\n",
       " 'details analysis part',\n",
       " 'also know tableau kind',\n",
       " 'negative articles release',\n",
       " 'draw solid conclusions',\n",
       " 'time online spend',\n",
       " 'midwood come project',\n",
       " 'always one symbol',\n",
       " '10 minutes',\n",
       " 'solid project overall',\n",
       " 'really impressive project',\n",
       " 'answers regarding studies',\n",
       " 'apartment type',\n",
       " 'single open question',\n",
       " 'would also like',\n",
       " 'also would like',\n",
       " 'people would prefer',\n",
       " 'sweet spot right',\n",
       " 'let participants participate',\n",
       " 'like really congratulations',\n",
       " 'google forms anymore',\n",
       " 'per se',\n",
       " 'use something running',\n",
       " 'positive negative correlation',\n",
       " 'highest salaries wanted',\n",
       " 'move industry great',\n",
       " 'really nice solution',\n",
       " 'like interesting enough',\n",
       " 'sample effect exists',\n",
       " 'always told like',\n",
       " 'important one followed',\n",
       " 'entire home',\n",
       " 'share something like',\n",
       " 'people really aim',\n",
       " 'really important part',\n",
       " 'apply many things',\n",
       " 'apply many things',\n",
       " 'also another category',\n",
       " 'analyzing anything technical',\n",
       " 'main differences',\n",
       " 'heard last week',\n",
       " 'average ticket',\n",
       " 'movie could bring',\n",
       " 'could better choose',\n",
       " 'revenue budget',\n",
       " 'get higher rewards',\n",
       " 'follow back casting',\n",
       " 'moving much closer',\n",
       " 'slide really well',\n",
       " 'people suspect anyway',\n",
       " 'general perception location',\n",
       " 'something similar happens',\n",
       " 'great analysis analysis',\n",
       " 'professional success mean',\n",
       " 'thousand dollars',\n",
       " 'normally says',\n",
       " 'survey also fun',\n",
       " 'see like many',\n",
       " 'per trial',\n",
       " 'learn another hypothesis',\n",
       " 'also showing like',\n",
       " 'goes really close',\n",
       " 'bar chart also',\n",
       " 'quick break break',\n",
       " 'get best return',\n",
       " 'help make like',\n",
       " 'queen next position',\n",
       " 'back end part',\n",
       " 'people would expect',\n",
       " 'stop participant sharing',\n",
       " 'python package used',\n",
       " 'awesome presentations next',\n",
       " 'like next projects',\n",
       " 'effort could make',\n",
       " 'really show like',\n",
       " 'maybe could affect',\n",
       " 'speed limits',\n",
       " 'speed limits',\n",
       " 'speed limits',\n",
       " 'speed limits',\n",
       " 'speed limits',\n",
       " 'speed limits',\n",
       " 'one else',\n",
       " 'weather condition',\n",
       " 'slot machine',\n",
       " 'power bi',\n",
       " 'conversion rate',\n",
       " 'main hypothesis',\n",
       " 'mean salingric level',\n",
       " 'mean salingric level',\n",
       " 'necessarily make movies',\n",
       " 'women prefer environment',\n",
       " 'save people belonging',\n",
       " 'build something visual',\n",
       " 'analysis specifically actually',\n",
       " 'main target',\n",
       " 'every month',\n",
       " 'stuff like yeah',\n",
       " 'speed limit',\n",
       " 'speed limit',\n",
       " 'speed limit',\n",
       " 'speed limit',\n",
       " 'create three profiles',\n",
       " 'would always tell',\n",
       " 'like taking care',\n",
       " 'would maybe content',\n",
       " 'would say like',\n",
       " 'would say like',\n",
       " 'informative next time',\n",
       " 'like facebook advertisement',\n",
       " 'like click',\n",
       " 'cinema chain',\n",
       " '25 dollar',\n",
       " 'stock goes low',\n",
       " 'user would want',\n",
       " 'good means 4',\n",
       " 'average price',\n",
       " '10 years',\n",
       " '10 years',\n",
       " 'three weeks',\n",
       " 'room type',\n",
       " 'would say target',\n",
       " 'strong modeling',\n",
       " 'learning condition',\n",
       " 'also think like',\n",
       " 'randomly take one',\n",
       " 'electric shocks',\n",
       " 'create really nice',\n",
       " 'airbnb listing',\n",
       " 'found something interesting',\n",
       " 'main dashboard',\n",
       " 'strong factor',\n",
       " 'strong factor',\n",
       " 'human factor',\n",
       " 'often drop',\n",
       " 'average casting',\n",
       " 'super impressive',\n",
       " 'less minutes',\n",
       " 'get things back',\n",
       " 'learned 19 questions',\n",
       " 'main topic',\n",
       " 'ab testing',\n",
       " 'main analysis',\n",
       " 'organic sales',\n",
       " 'week something whatever',\n",
       " 'reinforcement learning',\n",
       " 'reinforcement learning',\n",
       " 'reinforcement learning',\n",
       " 'reinforcement learning',\n",
       " 'main question',\n",
       " 'neighborhood groups',\n",
       " 'many hours',\n",
       " 'everything makes sense',\n",
       " 'little room',\n",
       " 'show one plot',\n",
       " 'email marketing',\n",
       " 'super cool',\n",
       " 'super cool',\n",
       " 'super cool',\n",
       " 'use one sample',\n",
       " 'total cost',\n",
       " 'technical components',\n",
       " 'technical components',\n",
       " 'main general',\n",
       " 'got one question',\n",
       " 'vegas nerve',\n",
       " 'stack overflow',\n",
       " 'motor bikes',\n",
       " 'missing operating',\n",
       " 'group activities',\n",
       " 'favorite actress',\n",
       " 'family reconciliation',\n",
       " 'energy activation',\n",
       " '350 euro',\n",
       " 'biggest driven',\n",
       " 'biggest driven',\n",
       " '50 answers',\n",
       " 'contingency test',\n",
       " 'contingency test',\n",
       " 'successful professional',\n",
       " 'successful professional',\n",
       " 'successful professional',\n",
       " 'main interest',\n",
       " 'took casting',\n",
       " 'digital nomads',\n",
       " 'control condition',\n",
       " 'pricing factor',\n",
       " 'pricing factor',\n",
       " 'pricing factor',\n",
       " 'several areas',\n",
       " 'successful professionals',\n",
       " 'successful professionals',\n",
       " 'successful professionals',\n",
       " 'strong correlation',\n",
       " 'strong data',\n",
       " 'say gonzalo saw',\n",
       " 'road condition',\n",
       " 'storytelling perspective',\n",
       " 'super islands',\n",
       " 'super helpful',\n",
       " 'super fast',\n",
       " 'super active',\n",
       " 'large one',\n",
       " 'condition one',\n",
       " 'urban areas',\n",
       " 'old young',\n",
       " 'average salary',\n",
       " 'two group',\n",
       " 'new incomes',\n",
       " 'example david gave',\n",
       " 'actual data',\n",
       " 'actual data',\n",
       " 'new stuff',\n",
       " 'modeling perception',\n",
       " 'deloitte single',\n",
       " 'private room',\n",
       " 'private room',\n",
       " 'private room',\n",
       " 'coming back',\n",
       " 'parents took',\n",
       " 'four blocks',\n",
       " 'annual incomes',\n",
       " '65 plus',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'data set',\n",
       " 'current speed',\n",
       " 'stuff together',\n",
       " 'recent correlation',\n",
       " 'whole paper',\n",
       " 'investment dates',\n",
       " 'every purchase',\n",
       " 'age range',\n",
       " 'age range',\n",
       " 'age range',\n",
       " 'car accidents',\n",
       " 'data cleaning',\n",
       " 'data cleaning',\n",
       " 'data cleaning',\n",
       " 'data cleaning',\n",
       " 'technical goals',\n",
       " 'pretty good',\n",
       " 'pretty good',\n",
       " 'pretty good',\n",
       " 'people make right',\n",
       " 'weather conditions',\n",
       " 'weather conditions',\n",
       " 'weather conditions',\n",
       " 'super low',\n",
       " 'original study',\n",
       " 'strong indicator',\n",
       " 'sham condition',\n",
       " 'super interesting',\n",
       " 'investment day',\n",
       " 'super powerful',\n",
       " 'new hypothesis',\n",
       " 'new hypothesis',\n",
       " 'new hypothesis',\n",
       " 'new hypothesis',\n",
       " 'significant 11',\n",
       " 'super clear',\n",
       " 'marketing strategy',\n",
       " 'marketing strategy',\n",
       " 'marketing strategy',\n",
       " 'specific person',\n",
       " 'every presentation',\n",
       " 'technical stuff',\n",
       " 'p values',\n",
       " 'word like',\n",
       " 'like fun',\n",
       " 'like creating',\n",
       " 'several panels',\n",
       " 'least giving',\n",
       " 'new entry',\n",
       " 'anyone wants',\n",
       " 'anyone wants',\n",
       " 'anyone wants',\n",
       " 'anyone wants',\n",
       " 'anyone wants',\n",
       " 'different movies',\n",
       " 'road accidents',\n",
       " 'big road',\n",
       " 'data gathering',\n",
       " 'data gathering',\n",
       " 'strongest factor',\n",
       " 'prime factor',\n",
       " 'analytical layer',\n",
       " 'additional factor',\n",
       " 'almost every',\n",
       " 'almost every',\n",
       " 'professional development',\n",
       " 'hypothesis testing',\n",
       " 'hypothesis testing',\n",
       " 'hypothesis testing',\n",
       " 'hypothesis testing',\n",
       " 'hypothesis testing',\n",
       " 'hypothesis testing',\n",
       " 'share room',\n",
       " 'share room',\n",
       " 'share room',\n",
       " 'super curious',\n",
       " 'whole city',\n",
       " 'whole city',\n",
       " 'barcelona month',\n",
       " 'first month',\n",
       " 'two years',\n",
       " 'showcase one',\n",
       " 'heard one',\n",
       " 'year 2019',\n",
       " 'wrapped together',\n",
       " 'visual cues',\n",
       " 'unbending task',\n",
       " 'stumbling block',\n",
       " 'spanish company',\n",
       " 'small person',\n",
       " 'small company',\n",
       " 'shipping cost',\n",
       " 'quite simple',\n",
       " 'proton type',\n",
       " 'oldest group',\n",
       " 'night base',\n",
       " 'multiple variables',\n",
       " 'motivation behind',\n",
       " 'landing perfectly',\n",
       " 'house type',\n",
       " 'group risks',\n",
       " 'group age',\n",
       " 'electrical signals',\n",
       " 'election worker',\n",
       " 'driving license',\n",
       " 'cost replace',\n",
       " 'confidence interval',\n",
       " 'age group',\n",
       " 'age group',\n",
       " 'age group',\n",
       " '448 rows',\n",
       " '30 p',\n",
       " 'super congratulations',\n",
       " 'best accommodation',\n",
       " 'different locations',\n",
       " 'actual presentation',\n",
       " 'single people',\n",
       " 'technical like',\n",
       " 'looking forward',\n",
       " 'fundamental data',\n",
       " 'testing independence',\n",
       " 'selling high',\n",
       " 'car like',\n",
       " 'professional success',\n",
       " 'professional success',\n",
       " 'professional success',\n",
       " 'professional success',\n",
       " 'professional success',\n",
       " 'pretty neat',\n",
       " 'pretty flawed',\n",
       " 'pretty bad',\n",
       " 'computer test',\n",
       " 'specific store',\n",
       " 'suspect something',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'little bit',\n",
       " 'like collecting',\n",
       " 'certain groups',\n",
       " 'really interactive',\n",
       " 'really interactive',\n",
       " 'many staff',\n",
       " 'many others',\n",
       " 'many opportunities',\n",
       " 'many clicks',\n",
       " 'something different',\n",
       " 'five days',\n",
       " 'total sales',\n",
       " 'two groups',\n",
       " 'two groups',\n",
       " 'two groups',\n",
       " 'two groups',\n",
       " 'revenue development',\n",
       " 'quite successful',\n",
       " 'lead us',\n",
       " 'either doctors',\n",
       " '24 layers',\n",
       " 'also provide',\n",
       " 'ppt presentation',\n",
       " 'group presentation',\n",
       " 'two movies',\n",
       " 'market strategy',\n",
       " 'one data',\n",
       " 'every song',\n",
       " 'specific day',\n",
       " 'different language',\n",
       " 'technical teams',\n",
       " 'partial outcome',\n",
       " 'every time',\n",
       " 'every time',\n",
       " 'prior experience',\n",
       " 'less switching',\n",
       " 'less precise',\n",
       " 'graphic accidents',\n",
       " 'big players',\n",
       " 'big paragraphs',\n",
       " 'big intersections',\n",
       " 'big intersections',\n",
       " 'big improvement',\n",
       " '17 accidents',\n",
       " 'conditions together',\n",
       " 'good students',\n",
       " 'good students',\n",
       " 'help set',\n",
       " 'help set',\n",
       " 'really dive',\n",
       " 'zero points',\n",
       " 'spanish government',\n",
       " 'small application',\n",
       " 'quick summary',\n",
       " 'quick inter',\n",
       " 'neurological responses',\n",
       " 'minor differences',\n",
       " 'last slide',\n",
       " 'last slide',\n",
       " 'dedicated areas',\n",
       " 'december 12th',\n",
       " 'came 930',\n",
       " '62 dollars',\n",
       " '6 dollars',\n",
       " 'tech',\n",
       " 'ink ratio',\n",
       " 'really good',\n",
       " 'really good',\n",
       " 'car accident',\n",
       " 'car accident',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "+# Keywords\n",
    "\n",
    "#RAKE\n",
    "from rake_nltk import Rake\n",
    "rake = Rake()\n",
    "rake.extract_keywords_from_text(result['text'])\n",
    "extracted_keyword = rake.get_ranked_phrases()\n",
    "extracted_keyword"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c2f1846f3955305d6cdcd7be5897be31bd89b0ce061940c94fabaf8ec721e2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
