{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "942876d5",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00005561c7d38550] main libvlc: Ejecutar vlc con la interfaz predeterminada. Use «cvlc» para usar vlc sin interfaz.\n",
      "[00007fe3ac004930] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
      "libva info: VA-API version 1.16.0\n",
      "libva error: vaGetDriverNameByIndex() failed with unknown libva error, driver_name = (null)\n",
      "[00007fe3ac004930] glconv_vaapi_x11 gl error: vaInitialize: unknown libva error\n",
      "libva info: VA-API version 1.16.0\n",
      "libva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so\n",
      "libva info: Found init function __vaDriverInit_1_14\n",
      "libva error: /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so init failed\n",
      "libva info: va_openDriver() returns 1\n",
      "libva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/i965_drv_video.so\n",
      "libva info: Found init function __vaDriverInit_1_10\n",
      "libva info: va_openDriver() returns 0\n",
      "[00007fe3ac004930] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
      "Failed to open VDPAU backend libvdpau_nvidia.so: no se puede abrir el archivo del objeto compartido: No existe el archivo o el directorio\n",
      "[00007fe3ac42d5c0] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
      "\u001b[31muint DBusMenuExporterDBus::GetLayout(int, int, const QStringList&, DBusMenuLayoutItem&)\u001b[0m: Condition failed: menu\n",
      "\u001b[31muint DBusMenuExporterDBus::GetLayout(int, int, const QStringList&, DBusMenuLayoutItem&)\u001b[0m: Condition failed: menu\n",
      "QObject::~QObject: Timers cannot be stopped from another thread\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OpenAI Whisper command line\n",
    "# To use the subtitles, can use: vlc test.mp4 --sub-file test2.srt\n",
    "subprocess.call(['vlc', 'data/test.mp4', '--sub-file', 'test2.srt'])\n",
    "# convert mp3 to wav file\n",
    "#%timeit subprocess.call(['whisper', 'data/test.mp3', '--model', 'tiny.en', '-o', 'data', '-f', 'txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca0e965b",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting video\n",
    "\n",
    "#Importing library and thir function\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "#reading from audio mp3 file\n",
    "sound = AudioSegment.from_mp3(\"/content/Audio/song_with_silence.mp3\")\n",
    "\n",
    "# spliting audio files\n",
    "audio_chunks = split_on_silence(sound, min_silence_len=500, silence_thresh=-40 )\n",
    "\n",
    "#loop is used to iterate over the output list\n",
    "for i, chunk in enumerate(audio_chunks):\n",
    "   output_file = \"/content/Audio/output/chunk{0}.mp3\".format(i)\n",
    "   print(\"Exporting file\", output_file)\n",
    "   chunk.export(output_file, format=\"mp3\")\n",
    "\n",
    "# chunk files saved as Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66617d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "audio = AudioSegment.from_file(\"data/test.mp4\", format=\"mp4\")\n",
    "\n",
    "chunks = split_on_silence(audio, \n",
    "                         keep_silence=30000, \n",
    "                         min_silence_len=200, \n",
    "                         silence_thresh=-30)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.export(f\"data/Chunks/output_{i}.mp3\", format=\"mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5887662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Transcription splitting video\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import whisper\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from threading import Thread\n",
    "\n",
    "sound = AudioSegment.from_file(\"/home/roque/01. IronHack/00. Data Analytics/01. Course/01. Week 1 - Day 1/Recordings/GMT20220906-165929_Recording_1920x1080.mp4\", format=\"mp4\")\n",
    "chunks = split_on_silence(\n",
    "    sound,\n",
    "\n",
    "    # split on silences longer than 1000ms (1 sec)\n",
    "    min_silence_len=1000,\n",
    "\n",
    "    # anything under -16 dBFS is considered silence\n",
    "    silence_thresh=-30, \n",
    "\n",
    "    # keep 200 ms of leading/trailing silence\n",
    "    keep_silence=200\n",
    ")\n",
    "\n",
    "# now recombine the chunks so that the parts are at least 30 sec long\n",
    "target_length = 30 * 1000\n",
    "output_chunks = [chunks[0]]\n",
    "for chunk in chunks[1:]:\n",
    "    if len(output_chunks[-1]) < target_length:\n",
    "        output_chunks[-1] += chunk\n",
    "    else:\n",
    "        # if the last output chunk is longer than the target length,\n",
    "        # we can start a new one\n",
    "        output_chunks.append(chunk)\n",
    "\n",
    "# now your have chunks that are bigger than 30 seconds (except, possibly the last one)\n",
    "\n",
    "n_threads = 8\n",
    "threads = []\n",
    "\n",
    "for i, chunk in enumerate(output_chunks):\n",
    "    name = f\"data/Chunks/output_{i}.mp3\"\n",
    "    format=\"mp3\"\n",
    "    t = Thread(target=chunk.export,args=(name, format))\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "\n",
    "# Get a list of all the audio files in the \"data/Chunks/\" folder\n",
    "audio_files = [f for f in sorted(os.listdir(\"data/Chunks/\")) if f.endswith('.mp3')]\n",
    "\n",
    "# Initialize an empty list to store the transcriptions\n",
    "transcriptions = []\n",
    "segments = []\n",
    "\n",
    "model = whisper.load_model(\"base.en\",device='cpu')\n",
    "#model = stable_whisper.load_model('base')\n",
    "\n",
    "# Loop over all the audio files in the \"data/Chunks/\" folderL_\n",
    "\n",
    "for audio_file in audio_files:\n",
    "    audio_file_path = os.path.join(\"data/Chunks/\", audio_file)\n",
    "    result = model.transcribe(audio_file_path)\n",
    "    #transcription = str(result)\n",
    "    transcriptions.append(result['text'])\n",
    "    #print(len(result['segments']))\n",
    "    #segments.append([res for res in result['segments']])\n",
    "    [segments.append(s) for s in result['segments']]\n",
    "    #segments.append(result['segments'])\n",
    "\n",
    "    os.remove(audio_file_path)\n",
    "\n",
    "with open(\"data/GMT20220906-165929_Recording_1920x1080.txt\", \"w+\") as f:\n",
    "    f.write(' '.join(transcriptions))\n",
    "\n",
    "# subtitles\n",
    "# TODO:Need to fix\n",
    "\n",
    "#segments_final = repairTranscriptSegments([seg for segment in segments for seg in segment],40)\n",
    "#segments_final = [s for seg in segments for s in seg]\n",
    "#segments = result['segments']\n",
    "\n",
    "\"\"\"ending = ''\n",
    "id_end = 0\n",
    "i = 0\n",
    "segments_final = []\n",
    "for segm in segments:         \n",
    "    if (i > 0):        \n",
    "        if (segm['id'] == i):\n",
    "            i = segm['id']\n",
    "        else:\n",
    "            segm['id'] = i\n",
    "            print(segments[i-1]['end'])\n",
    "            segm['start'] = round(float(segm['start']+segments[i-1]['end']),2)\n",
    "            segm['end'] = round(float(segm['end']+segments[i-1]['end']),2)\n",
    "  \n",
    "    segments_final.append(segm)\n",
    "  \n",
    "    i += 1      \n",
    "\n",
    "for seg in segments_final:    \n",
    "    startTime = str(0)+str(timedelta(seconds=int(seg['start'])))+',000'\n",
    "    endTime = str(0)+str(timedelta(seconds=int(seg['end'])))+',000'\n",
    "    text = seg['text']\n",
    "    segmentId = seg['id']+1\n",
    "    segmentFinal = f\"{segmentId}\\n{startTime} --> {endTime}\\n{text[1:] if text[0] == ' ' else text}\\n\\n\"\n",
    "\n",
    "    srtFilename = os.path.join(r\"data\", \"test.srt\")\n",
    "    with open(srtFilename, 'a', encoding='utf-8') as srtFile:\n",
    "        srtFile.write(segmentFinal)\n",
    "\"\"\"    \n",
    "\n",
    "# subtitles \n",
    "result_final = {}\n",
    "result_final['text'] = ' '.join(transcriptions)\n",
    "result_final['segments'] = segments\n",
    "result_final['language'] = 'en'\n",
    "stable_whisper.results_to_sentence_srt(result_final, \"data/GMT20220906-165929_Recording_1920x1080.srt\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f47ad38",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "284bbe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "#summarization\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "\n",
    "text = open('data/GMT20221029-085534_Recording_1920x1080.txt').read()\n",
    "\n",
    "model = 'pszemraj/led-large-book-summary'\n",
    "\n",
    "#summarizer = pipeline(\"summarization\", model, truncation=True)\n",
    "summarizer = pipeline(\"summarization\", truncation=True)\n",
    "mysummary = summarizer(text, min_length=100, max_length=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f1dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysummary[0]['summary_text'])\n",
    "print('<<<<>>>>')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60cc91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nest_sentences(document):\n",
    "\n",
    "  nested = []\n",
    "  sent = []\n",
    "  length = 0\n",
    "  for sentence in nltk.sent_tokenize(document):\n",
    "    length += len(sentence)\n",
    "    if length < 1024:\n",
    "      sent.append(sentence)\n",
    "    else:\n",
    "      nested.append(sent)\n",
    "      sent = []\n",
    "      length = 0\n",
    "\n",
    "  if sent:\n",
    "    nested.append(sent)\n",
    "\n",
    "  return nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44e9e640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested = nest_sentences(text)\n",
    "len(nested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d53568cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a67d6b4d23e442d9e6c033d1a40bb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32b165df7d94e89b91ab40818b4ee6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "import torch\n",
    "\n",
    "BART_PATH = 'facebook/bart-large-cnn'\n",
    "\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(BART_PATH, output_past=True)\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(BART_PATH, output_past=True)\n",
    "\n",
    "def generate_summary(nested_sentences):\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  \n",
    "  summaries = []\n",
    "  for nested in nested_sentences:\n",
    "    input_tokenized = bart_tokenizer.encode(' '.join(nested), truncation=True, return_tensors='pt')\n",
    "    input_tokenized = input_tokenized.to(device)\n",
    "    summary_ids = bart_model.to(device).generate(input_tokenized,\n",
    "                                      length_penalty=3.0,\n",
    "                                      min_length=30,\n",
    "                                      max_length=100)\n",
    "    output = [bart_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    summaries.append(output)\n",
    "  summaries = [sentence for sublist in summaries for sentence in sublist]\n",
    "  return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6866f5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summ \u001b[39m=\u001b[39m generate_summary(nested)\n",
      "Cell \u001b[0;32mIn[46], line 15\u001b[0m, in \u001b[0;36mgenerate_summary\u001b[0;34m(nested_sentences)\u001b[0m\n\u001b[1;32m     13\u001b[0m input_tokenized \u001b[39m=\u001b[39m bart_tokenizer\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(nested), truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m input_tokenized \u001b[39m=\u001b[39m input_tokenized\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m summary_ids \u001b[39m=\u001b[39m bart_model\u001b[39m.\u001b[39;49mto(device)\u001b[39m.\u001b[39;49mgenerate(input_tokenized,\n\u001b[1;32m     16\u001b[0m                                   length_penalty\u001b[39m=\u001b[39;49m\u001b[39m3.0\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m                                   min_length\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m                                   max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m output \u001b[39m=\u001b[39m [bart_tokenizer\u001b[39m.\u001b[39mdecode(g, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m summary_ids]\n\u001b[1;32m     20\u001b[0m summaries\u001b[39m.\u001b[39mappend(output)\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/transformers/generation/utils.py:1474\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1468\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1469\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1470\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1471\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1475\u001b[0m         input_ids,\n\u001b[1;32m   1476\u001b[0m         beam_scorer,\n\u001b[1;32m   1477\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1478\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1479\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1480\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1481\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1482\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1483\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1484\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1485\u001b[0m     )\n\u001b[1;32m   1487\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1488\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/transformers/generation/utils.py:2722\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2718\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2722\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2723\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2724\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2725\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2726\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2727\u001b[0m )\n\u001b[1;32m   2729\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2730\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:1374\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1371\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1372\u001b[0m         )\n\u001b[0;32m-> 1374\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1375\u001b[0m     input_ids,\n\u001b[1;32m   1376\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1377\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1378\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   1379\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1380\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1381\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1382\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1383\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1384\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1385\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1386\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1387\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1388\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1389\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1390\u001b[0m )\n\u001b[1;32m   1392\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\n\u001b[1;32m   1393\u001b[0m lm_logits \u001b[39m=\u001b[39m lm_logits \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\u001b[39m.\u001b[39mto(lm_logits\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:1256\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1250\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   1251\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1252\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1253\u001b[0m     )\n\u001b[1;32m   1255\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1257\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1258\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1259\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m   1260\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1261\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1262\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1263\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1264\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1265\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1266\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1267\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1268\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1269\u001b[0m )\n\u001b[1;32m   1271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1272\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:1113\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m   1102\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m   1103\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1110\u001b[0m     )\n\u001b[1;32m   1111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1113\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1114\u001b[0m         hidden_states,\n\u001b[1;32m   1115\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1116\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1117\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1118\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1119\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[1;32m   1120\u001b[0m             cross_attn_head_mask[idx] \u001b[39mif\u001b[39;49;00m cross_attn_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m   1121\u001b[0m         ),\n\u001b[1;32m   1122\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1123\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1124\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1125\u001b[0m     )\n\u001b[1;32m   1126\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:445\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m    444\u001b[0m cross_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_attn(\n\u001b[1;32m    446\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    447\u001b[0m     key_value_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    448\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    449\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m    450\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mcross_attn_past_key_value,\n\u001b[1;32m    451\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    452\u001b[0m )\n\u001b[1;32m    453\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    454\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/01. IronHack/00. Data Analytics/01. Course/63. Week 23 - Day 3/git/final-project-bootcamp/.venv/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:236\u001b[0m, in \u001b[0;36mBartAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    233\u001b[0m value_states \u001b[39m=\u001b[39m value_states\u001b[39m.\u001b[39mview(\u001b[39m*\u001b[39mproj_shape)\n\u001b[1;32m    235\u001b[0m src_len \u001b[39m=\u001b[39m key_states\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(query_states, key_states\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m))\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m attn_weights\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, src_len):\n\u001b[1;32m    239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttention weights should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\u001b[39m \u001b[39mtgt_len,\u001b[39m \u001b[39msrc_len)\u001b[39m}\u001b[39;00m\u001b[39m, but is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mattn_weights\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "summ = generate_summary(nested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nested_summ = nest_sentences(' '.join(summ))\n",
    "nested_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_summary(nested_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "436557c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to mysql\n",
    "import mysql.connector\n",
    "\n",
    "#Operating system\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "#import subprocess\n",
    "from datetime import timedelta\n",
    "from threading import Thread\n",
    "\n",
    "#Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "933cd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_ddbb():\n",
    "    \"\"\"Function to create the conection to the data base\n",
    "    \n",
    "    Keyword arguments:    \n",
    "    Return: connection objet, secrets object\n",
    "    \"\"\"\n",
    "       \n",
    "    secrets={}\n",
    "    #secrets_file = open('secrets.txt','r') #Had errors using this within a virtual environment\n",
    "    secrets_file = os.fdopen(os.open('secrets.txt', os.O_RDONLY))\n",
    "    for line in secrets_file:\n",
    "        (key, val) = line.replace('\\n','').split(\"|\")\n",
    "        secrets[key] = val    \n",
    "\n",
    "    #Conection to mysql\n",
    "\n",
    "    conn = mysql.connector.connect(user=secrets['user'],\n",
    "                            password=secrets['pass'],\n",
    "                            host=secrets['server'])\n",
    "    \n",
    "    return conn, secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "765144e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection, secrets = connection_ddbb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "764cd997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "query = \"\"\"SELECT transcription\n",
    "                    FROM ironrep.transcriptions\n",
    "                WHERE videoid = %s\n",
    "                    AND languageid = %s\"\"\"\n",
    "val = [int(45), 'en']\n",
    "cursor.execute(query, val)    \n",
    "subt_table = cursor.fetchall()\n",
    "if (len(subt_table)>0):\n",
    "    subt_df = pd.DataFrame(subt_table)\n",
    "    subt_df.columns = [i[0] for i in cursor.description]\n",
    "    transcription = subt_df['transcription'][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1349f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transcription)\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7030a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#hub_model_id = \"JulesBelveze/t5-small-headline-generator\" #OK for headline 11sg\n",
    "#hub_model_id = \"ybagoury/flan-t5-base-tldr_news\" #Interesting but maybe not 1m19sg\n",
    "hub_model_id = \"mrm8488/flan-t5-large-finetuned-openai-summarize_from_feedback\" #OK Muy bueno 1m\n",
    "summarizer = pipeline(\"summarization\", model=hub_model_id)\n",
    "summary = summarizer(' '.join(transcription.split(' ')[:250]), max_length=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bc01c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"I'm recording the whole idea behind hypothesis testing. Let's talk about what goodness of fit is and how we can use it. Here's what we've learned\"}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hub_model_id = \"JulesBelveze/t5-small-headline-generator\" #OK for headline\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8c919e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"I'm going to talk about what goodness of fit is and how we can use it. So let's talk about the whole idea behind hypothesis testing. We have some hypothesis that we want to test, hence the name, which is something that can either be true or false. And the way we go about it is we test it. And we say, well, under one of these hypothesis, the one we call a null hypothesis, how likely is it to observe these sample that we actually have extracted from reality. And if what we observed from our sampling of reality, straight too far away from what our nul hypothesis would say, we say , okay, there is no evidence here to reject this this preconceived idea that we have against the world about the world \"}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hub_model_id = \"ybagoury/flan-t5-base-tldr_news\" Interesting but maybe not\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06063d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"you take a sample of reality and you say, if it is consistent with your null hypothesis, then you say, okay, I'd rather believe that our null hypothesis was false, then to believe that I live in a world where I was so unlucky that I witnessed this sample that straced so far away from what seems to be possible.\"}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hub_model_id = \"mrm8488/flan-t5-large-finetuned-openai-summarize_from_feedback\" \n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ab01c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"you take a sample of reality and you say, if it is consistent with your null hypothesis, then you say, okay, I'd rather believe that our null hypothesis was false, then to believe that I live in a world where I was so unlucky that I witnessed this sample that straced so far away from what seems to be possible.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667bf27",
   "metadata": {},
   "source": [
    "' '.join(transcription.split(' ')[:250])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c2f1846f3955305d6cdcd7be5897be31bd89b0ce061940c94fabaf8ec721e2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
